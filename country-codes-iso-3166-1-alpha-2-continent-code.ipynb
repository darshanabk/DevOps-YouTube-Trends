{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b05f2a",
   "metadata": {
    "_cell_guid": "cc3e7e5a-6668-4e7a-9787-b5766ad05e69",
    "_uuid": "69d08faa-93f0-49ff-9b85-09e2d17becf0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002708,
     "end_time": "2025-02-03T14:04:27.706482",
     "exception": false,
     "start_time": "2025-02-03T14:04:27.703774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Country Codes & Continents: A Dataset with ISO 3166-1 Alpha-2\n",
    "\n",
    "This notebook aims to create a dataset of countries, their corresponding ISO 3166-1 Alpha-2 codes, and their respective continents.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* Utilizes the ISO 3166-1 Alpha-2 standard for country codes.\n",
    "* Includes a comprehensive list of countries and their associated continents.\n",
    "* Provides a clean and organized dataset for various data analysis and mapping projects.\n",
    "\n",
    "**Potential Use Cases:**\n",
    "\n",
    "* Geocoding and mapping applications.\n",
    "* Data analysis and visualization projects.\n",
    "* Internationalization and localization tasks.\n",
    "* Building applications that require country-specific information.\n",
    "\n",
    "This notebook demonstrates a simple and efficient approach to gathering and organizing country-related data.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "* Data sources may vary, and the accuracy of the information should be verified independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f11392",
   "metadata": {
    "_cell_guid": "5a7208e7-0b25-4761-884b-22293ca8f31a",
    "_uuid": "1c036095-8ec1-4a9c-a64b-31e8cbf1d2a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T14:04:27.712873Z",
     "iopub.status.busy": "2025-02-03T14:04:27.712367Z",
     "iopub.status.idle": "2025-02-03T14:04:27.736217Z",
     "shell.execute_reply": "2025-02-03T14:04:27.734793Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02931,
     "end_time": "2025-02-03T14:04:27.738188",
     "exception": false,
     "start_time": "2025-02-03T14:04:27.708878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updated function to fetch continent from Wikipedia by parsing the infobox\n",
    "def fetch_continent_from_wikipedia(country_name):\n",
    "    \"\"\"\n",
    "    Scrape Wikipedia to determine the continent of a country by parsing its infobox.\n",
    "    This function looks for a row with a header containing 'Continent' and then checks \n",
    "    the cell text against known continent keywords.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_url = f\"https://en.wikipedia.org/wiki/{country_name.replace(' ', '_')}\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        \n",
    "        # Define continent keywords and corresponding codes\n",
    "        continent_keywords = {\n",
    "            \"Africa\": \"AF\",\n",
    "            \"Asia\": \"AS\",\n",
    "            \"Europe\": \"EU\",\n",
    "            \"North America\": \"NA\",\n",
    "            \"South America\": \"SA\",\n",
    "            \"Oceania\": \"OC\",\n",
    "            \"Antarctica\": \"AN\"\n",
    "        }\n",
    "        \n",
    "        # Try to locate the infobox table\n",
    "        infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "        if infobox:\n",
    "            for row in infobox.find_all(\"tr\"):\n",
    "                header = row.find(\"th\")\n",
    "                if header and \"Continent\" in header.text:\n",
    "                    cell = row.find(\"td\")\n",
    "                    if cell:\n",
    "                        text = cell.get_text(separator=\" \", strip=True)\n",
    "                        for key, code in continent_keywords.items():\n",
    "                            if key in text:\n",
    "                                return code\n",
    "        # Fallback: search the entire page text\n",
    "        page_text = soup.get_text()\n",
    "        for key, code in continent_keywords.items():\n",
    "            if key in page_text:\n",
    "                return code\n",
    "        return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def get_continent_code(alpha2, country_name):\n",
    "    \"\"\"\n",
    "    First, attempt to get the continent code using pycountry_convert.\n",
    "    If that fails or the result is ambiguous, scrape Wikipedia for a more accurate value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        continent_code_pc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "    except Exception:\n",
    "        continent_code_pc = None\n",
    "    \n",
    "    scraped_code = fetch_continent_from_wikipedia(country_name)\n",
    "    \n",
    "    # Prefer the scraped code if available; otherwise, use the pycountry_convert result.\n",
    "    if scraped_code != \"Unknown\":\n",
    "        return scraped_code\n",
    "    elif continent_code_pc is not None:\n",
    "        return continent_code_pc\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def ISO_3166_1_Alpha_2(it_hub_regions, continent_name_mapping):\n",
    "    \"\"\"\n",
    "    Fetch ISO country data from Wikipedia and, for each country:\n",
    "      - Extract its ISO 3166-1 Alpha-2 code and name.\n",
    "      - Determine the continent code (using automated methods).\n",
    "      - Map the continent code to the full continent name.\n",
    "      - Determine if it is an IT hub based on a predefined list.\n",
    "    This fully automated approach avoids manual customizations.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": \"ISO_3166-1_alpha-2\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"text\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    page_content = data.get(\"parse\", {}).get(\"text\", {}).get(\"*\")\n",
    "    if not page_content:\n",
    "        raise ValueError(\"Failed to fetch data from Wikipedia.\")\n",
    "    \n",
    "    soup = BeautifulSoup(page_content, \"lxml\")\n",
    "    iso_data = {}\n",
    "    \n",
    "    # Locate the \"Officially assigned code elements\" header\n",
    "    header = soup.find('h3', id='Officially_assigned_code_elements')\n",
    "    if not header:\n",
    "        raise ValueError(\"Section with id 'Officially_assigned_code_elements' not found.\")\n",
    "    \n",
    "    table = header.find_next(\"table\")\n",
    "    if not table:\n",
    "        raise ValueError(\"Table not found after the header.\")\n",
    "    \n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Extract ISO alpha-2 country code.\n",
    "        code_span = cells[0].find('span', class_=\"monospaced\")\n",
    "        key = code_span.text.strip() if code_span else \"\"\n",
    "        \n",
    "        # Extract country name.\n",
    "        link = cells[1].find('a')\n",
    "        value = link.text.strip() if link else \"\"\n",
    "        \n",
    "        if key:\n",
    "            continent_cd = get_continent_code(key, value)\n",
    "            continent_name = continent_name_mapping.get(continent_cd, \"Unknown\")\n",
    "            it_hub_status = \"Yes\" if key in it_hub_regions else \"No\"\n",
    "            \n",
    "            iso_data[key] = {\n",
    "                \"country_name\": value,\n",
    "                \"continent\": continent_name,\n",
    "                \"continent_code\": continent_cd,\n",
    "                \"it_hub_country\": it_hub_status\n",
    "            }\n",
    "    \n",
    "    return iso_data\n",
    "\n",
    "def RawFile(it_hub_regions, continent_name_mapping):\n",
    "    \"\"\"\n",
    "    Generate and save a JSON file containing the structured country details.\n",
    "    The filename includes a timestamp to ensure uniqueness.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dictionary = ISO_3166_1_Alpha_2(it_hub_regions, continent_name_mapping)\n",
    "        if dictionary:\n",
    "            timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "            filename = f\"R_{timestamp}_country_details.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(dictionary, file, indent=4, ensure_ascii=False)\n",
    "            print(f\"Dictionary saved as {filename}\")\n",
    "        else:\n",
    "            print(\"No data to save since empty dictionary returned.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error during raw file creation: {e}\")\n",
    "        return False\n",
    "\n",
    "def PushToGithub(repo_url):\n",
    "    \"\"\"\n",
    "    Automate pushing the generated JSON file to GitHub.\n",
    "    Finds the latest JSON file, clones or pulls the target repository,\n",
    "    copies the file into the repositoryâ€™s daily directory, commits, and pushes.\n",
    "    \"\"\"\n",
    "    output_files = os.listdir(os.getcwd())\n",
    "    try:\n",
    "        json_files = [file for file in output_files if file.startswith(\"R_\") and file.endswith(\"_country_details.json\")]\n",
    "        if json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON files found!\")\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred at fetching recent .json file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    kaggle_repo_url = os.path.join(os.getcwd(), \"YouTubeFoodChannelAnalysis\")\n",
    "    destination_path = os.path.join(kaggle_repo_url, \"Requirement\", \"Daily\")\n",
    "    \n",
    "    print(f\"Latest JSON file: {latest_file}\")\n",
    "    try:\n",
    "        if os.path.exists(kaggle_repo_url):\n",
    "            print(\"Repository already cloned; pulling latest changes.\")\n",
    "            repo = git.Repo(kaggle_repo_url)\n",
    "            origin = repo.remote(name='origin')\n",
    "            origin.pull()\n",
    "            print(\"Successfully pulled the latest changes.\")\n",
    "        else:\n",
    "            repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "            print(\"Successfully cloned the repository.\")\n",
    "        \n",
    "        if not os.path.exists(destination_path):\n",
    "            os.makedirs(destination_path)\n",
    "        shutil.copyfile(os.path.join(os.getcwd(), latest_file),\n",
    "                        os.path.join(destination_path, latest_file))\n",
    "        \n",
    "        repo = Repo(kaggle_repo_url)\n",
    "        repo.index.add([os.path.join(destination_path, latest_file)])\n",
    "        timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        repo.index.commit(f\"{timestamp} - Added {latest_file} from Kaggle notebook\")\n",
    "        origin = repo.remote(name=\"origin\")\n",
    "        push_result = origin.push()\n",
    "        if push_result:\n",
    "            print(\"Output files successfully pushed to GitHub!\")\n",
    "        else:\n",
    "            print(\"Pushing to GitHub failed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during git automation: {e}\")\n",
    "        return False\n",
    "\n",
    "def main(repo_url, it_hub_regions, continent_name_mapping):\n",
    "    RawFile(it_hub_regions, continent_name_mapping)\n",
    "    PushToGithub(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe94003",
   "metadata": {
    "_cell_guid": "3af78874-4501-4beb-8343-d444c3fd068f",
    "_uuid": "eb4989db-ad63-4ddd-8bc5-a316490cfe96",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T14:04:27.743461Z",
     "iopub.status.busy": "2025-02-03T14:04:27.743146Z",
     "iopub.status.idle": "2025-02-03T14:06:53.356282Z",
     "shell.execute_reply": "2025-02-03T14:06:53.355021Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 145.617893,
     "end_time": "2025-02-03T14:06:53.358284",
     "exception": false,
     "start_time": "2025-02-03T14:04:27.740391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved as R_2025-02-03_19:36:50_country_details.json\n",
      "Latest JSON file: R_2025-02-03_19:36:50_country_details.json\n",
      "Successfully cloned the repository.\n",
      "Output files successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pycountry_convert as pc\n",
    "    import datetime, os, json, shutil, git\n",
    "    from git import Repo\n",
    "    from pytz import timezone\n",
    "    from IPython.display import display, JSON\n",
    "    import pandas as pd\n",
    "    # Setup and import necessary modules\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"requirementRepoUrl\")  # Fetch repository URL\n",
    "    repo_url = secret_value_0\n",
    "    \n",
    "    # Set timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Define IT hub regions (automated list)\n",
    "    it_hub_regions = [\n",
    "        'US', 'IN', 'CN', 'JP', 'KR', 'DE', 'GB', 'FR', 'CA', 'AU',\n",
    "        'SG', 'SE', 'FI', 'IE', 'IL', 'NL', 'CH', 'ES', 'IT', 'BR', \n",
    "        'ZA', 'RU', 'AE', 'TR', 'PL', 'VN', 'MY', 'PH', 'TH', 'ID', \n",
    "        'HK', 'TW',\n",
    "    ]\n",
    "    \n",
    "    # Automated mapping from continent codes to full names.\n",
    "    continent_name_mapping = {\n",
    "        \"AF\": \"Africa\",\n",
    "        \"AS\": \"Asia\",\n",
    "        \"EU\": \"Europe\",\n",
    "        \"NA\": \"North America\",\n",
    "        \"SA\": \"South America\",\n",
    "        \"OC\": \"Oceania\",\n",
    "        \"AN\": \"Antarctica\"\n",
    "    }\n",
    "    \n",
    "    main(repo_url, it_hub_regions, continent_name_mapping)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 149.175217,
   "end_time": "2025-02-03T14:06:54.082669",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T14:04:24.907452",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
