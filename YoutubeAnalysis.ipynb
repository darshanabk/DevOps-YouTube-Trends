{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SourceDaily\n\n* The script processes records by extracting video details using the YouTube API.\n* It fetches video metadata such as titles, descriptions, view counts etc based on specific keywords.","metadata":{}},{"cell_type":"code","source":"import datetime\nfrom pytz import timezone\nimport pandas as pd\n\n# Initialize an empty DataFrame with required columns\nFileExecution = pd.DataFrame(columns=['ScriptFile', 'StartTime', 'EndTime', 'TimeTaken', 'Date'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:36.566196Z","iopub.execute_input":"2025-01-22T08:53:36.566602Z","iopub.status.idle":"2025-01-22T08:53:36.574342Z","shell.execute_reply.started":"2025-01-22T08:53:36.566570Z","shell.execute_reply":"2025-01-22T08:53:36.573056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Recording the start time of execution\nstart_time = datetime.datetime.now()\n# Code block for which execution time need to measure\nprint(\"Execution started...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:36.575937Z","iopub.execute_input":"2025-01-22T08:53:36.576341Z","iopub.status.idle":"2025-01-22T08:53:36.601288Z","shell.execute_reply.started":"2025-01-22T08:53:36.576298Z","shell.execute_reply":"2025-01-22T08:53:36.600050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.595954Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.596405Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.606814Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.596363Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.605597Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef VideoDetailExtraction(kw_list, maxResults=50):\n    \"\"\"\n    Fetches a list of video details from YouTube based on the given keyword(s) for the initial batch.\n\n    Args:\n        kw_list (str): The keyword(s) to search for.\n        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n\n    Returns:\n        dict: The API response containing video details. Returns None if an error occurs.\n    \"\"\"\n    try:\n        # Prepare the API request to fetch video details\n        request = youtube.search().list(\n            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n            order='viewCount',         # Order results by view count\n            q=kw_list,                 # Search query\n            relevanceLanguage='en',    # Limit results to English-relevant videos\n            type='video',              # Restrict results to videos only\n            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n        )\n\n        # Execute the API request\n        response = request.execute()\n        return response\n\n    except Exception as e:\n        # Log any errors encountered during the API call\n        print(f\"Error during VideoDetailExtraction(): {e}\")\n        return None\n\n\ndef VideoDetailExtractionNextPageToken(kw_list, nextPageToken, maxResults=50):\n    \"\"\"\n    Fetches the next page of video details from YouTube using a continuation token.\n\n    Args:\n        kw_list (str): The keyword(s) to search for.\n        nextPageToken (str): The token for fetching the next page of results.\n        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n\n    Returns:\n        dict: The API response containing video details for the next page. Returns None if an error occurs.\n    \"\"\"\n    try:\n        # Prepare the API request to fetch the next page of video details\n        request = youtube.search().list(\n            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n            order='viewCount',         # Order results by view count\n            q=kw_list,                 # Search query\n            relevanceLanguage='en',    # Limit results to English-relevant videos\n            type='video',              # Restrict results to videos only\n            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n            pageToken=nextPageToken,   # Token for fetching the next page\n            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n        )\n\n        # Execute the API request\n        response = request.execute()\n        return response\n\n    except Exception as e:\n        # Log any errors encountered during the API call\n        print(f\"Error during VideoDetailExtractionNextPageToken(): {e}\")\n        return None\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.761981Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.762320Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.784360Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.762295Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.783267Z\"}}\ndef VideoDataFrame(response):\n    \"\"\"\n    Processes video and channel details from the YouTube API response, structures the data into DataFrames,\n    and merges them to create a comprehensive dataset.\n\n    Args:\n        response (dict): The response object returned by the YouTube API containing video details.\n\n    Returns:\n        tuple:\n            - pd.DataFrame: A DataFrame containing merged video and channel details.\n            - str or None: The next page token if available, otherwise None.\n    \"\"\"\n    try:\n        # Initialize lists to store video and channel details\n        videoDetails = []\n        videoIds = []\n        channelIds = []\n        channelDetails = []\n        \n        '''\n        Video Search Block: Extract basic video details from the response.\n        '''\n        for i in range(len(response['items'])):\n            # Extract publication time and convert to components\n            publishedOn = response['items'][i].get('snippet', '0000-00-00T00:00:00Z').get('publishTime', '0000-00-00T00:00:00Z')\n            publishTime = re.split(r'[TZ-]', publishedOn)\n            total_seconds = 0\n            if publishedOn != '0000-00-00T00:00:00Z':\n                try:\n                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n                except ValueError:\n                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n                epoch = datetime.datetime(1970, 1, 1)\n                total_seconds = int((dt - epoch).total_seconds())\n            \n            # Append extracted video details\n            videoDetails.append({\n                'channelId': response['items'][i]['snippet']['channelId'],\n                'channelName': response['items'][i]['snippet']['channelTitle'],\n                'videoId': response['items'][i]['id']['videoId'],\n                'videoTitle': response['items'][i]['snippet']['title'],\n                'publishYear': publishTime[0],  # Extracted year\n                'publishMonth': publishTime[1],  # Extracted month\n                'publishDay': publishTime[2],  # Extracted day\n                'publishTime': publishTime[3],  # Extracted time\n                'publishedOn': publishedOn,\n                'publishedOnInSeconds': total_seconds\n            })\n            \n            # Collect video and channel IDs\n            videoIds.append(response['items'][i]['id']['videoId'])\n            channelIds.append(response['items'][i]['snippet']['channelId'])\n        \n        # Extract next page token if available\n        nextPageToken = response.get(\"nextPageToken\", None)\n        \n        '''\n        Video Block: Fetch additional details about each video using its ID.\n        '''\n        request = youtube.videos().list(\n            part='id,statistics,snippet,contentDetails,localizations,status,liveStreamingDetails,paidProductPlacementDetails,player,recordingDetails,topicDetails',\n            id=videoIds\n        )\n        response = request.execute()\n        \n        for i in range(len(response['items'])):\n            video = response['items'][i]\n            \n            # Video statistics\n            statistics = video.get('statistics', {})\n            videoDetails[i]['videoViewCount'] = statistics.get('viewCount', 0)\n            videoDetails[i]['videoLikeCount'] = statistics.get('likeCount', 0)\n            videoDetails[i]['videoFavoriteCount'] = statistics.get('favoriteCount', 0)\n            videoDetails[i]['videoCommentCount'] = statistics.get('commentCount', 0)\n            \n            # Video snippet details\n            snippet = video.get('snippet', {})\n            videoDetails[i]['videoDescription'] = snippet.get('description', None)\n            videoDetails[i]['videoTags'] = snippet.get('tags', [])\n            videoDetails[i]['videoCategoryId'] = snippet.get('categoryId', None)\n            videoDetails[i]['videoLiveBroadcastContent'] = snippet.get('liveBroadcastContent', None)\n            videoDetails[i]['videoDefaultLanguage'] = snippet.get('defaultLanguage', None)\n            videoDetails[i]['videoDefaultAudioLanguage'] = snippet.get('defaultAudioLanguage', None)\n            \n            # Video duration (convert ISO 8601 to seconds)\n            duration = video.get('contentDetails', {}).get('duration', None)\n            if duration:\n                match = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", duration)\n                hours = int(match.group(1) or 0)\n                minutes = int(match.group(2) or 0)\n                seconds = int(match.group(3) or 0)\n                videoDetails[i]['videoDuration'] = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n                videoDetails[i]['videoDurationInSeconds'] = hours * 3600 + minutes * 60 + seconds\n            else:\n                videoDetails[i]['videoDuration'] = None\n                videoDetails[i]['videoDurationInSeconds'] = None\n            \n            # Additional video details\n            content_details = video.get('contentDetails', {})\n            videoDetails[i]['videoDimension'] = content_details.get('dimension', None)\n            videoDetails[i]['videoDefinition'] = content_details.get('definition', None)\n            videoDetails[i]['videoCaption'] = content_details.get('caption', None)\n            videoDetails[i]['videoLicensedContent'] = content_details.get('licensedContent', False)\n            videoDetails[i]['videoProjection'] = content_details.get('projection', False)\n        \n        '''\n        Channel Block: Fetch details for channels associated with the videos.\n        '''\n        videoDetails = pd.DataFrame(videoDetails)\n        Unique_ChannelIds = list(set(videoDetails['channelId']))\n        \n        request = youtube.channels().list(\n            part='id,contentDetails,brandingSettings,contentOwnerDetails,localizations,snippet,statistics,status,topicDetails',\n            id=Unique_ChannelIds\n        )\n        response = request.execute()\n        \n        for i in range(len(response['items'])):\n            item = response['items'][i]\n            snippet = item.get('snippet', {})\n            publishedOn = snippet.get('publishedAt', '0000-00-00T00:00:00Z')\n            publishedAt = re.split(r'[TZ-]', publishedOn)\n            total_seconds = 0\n            if publishedOn != '0000-00-00T00:00:00Z':\n                try:\n                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n                except ValueError:\n                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n                epoch = datetime.datetime(1970, 1, 1)\n                total_seconds = int((dt - epoch).total_seconds())\n            \n            # Extract channel details\n            channelDetails.append({\n                'channelIdUnique': item['id'],\n                'channelTitleCheck': snippet.get('title', None),\n                'channelDescription': snippet.get('description', None),\n                'channelCustomUrl': snippet.get('customUrl', None),\n                'channelPublishYear': publishedAt[0],\n                'channelPublishMonth': publishedAt[1],\n                'channelPublishDay': publishedAt[2],\n                'channelPublishTime': publishedAt[3],\n                'channelPublishedOn': publishedOn,\n                'channelPublishedOnInSeconds': total_seconds,\n                'channelCountry': snippet.get('country', None),\n                'channelViewCount': item.get('statistics', {}).get('viewCount', 0),\n                'channelSubscriberCount': item.get('statistics', {}).get('subscriberCount', 0),\n                'channelVideoCount': item.get('statistics', {}).get('videoCount', 0),\n            })\n        \n        # Convert channel details to DataFrame\n        channelDetails = pd.DataFrame(channelDetails)\n        \n        '''\n        Result: Merge video and channel details into a single DataFrame.\n        '''\n        resultDataFrame = pd.merge(videoDetails, channelDetails, left_on='channelId', right_on='channelIdUnique', how='left')\n        return resultDataFrame, nextPageToken\n    \n    except Exception as e:\n        print(f\"Error while processing VideoDataFrame(): {e}\")\n        return None, None\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.785758Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.786157Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.807347Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.786116Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.806005Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef VideoDetailsStructuring(max_record_count, kw_list):\n    \"\"\"\n    Fetches and structures video details into a DataFrame, handling pagination if necessary.\n\n    Args:\n        max_record_count (int): The maximum number of video records to fetch.\n        kw_list (str): The keyword(s) to use for fetching video details.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing video details. Returns an empty DataFrame on failure.\n    \"\"\"\n    try:\n        # Initialize an empty DataFrame to store results\n        resultDataFrame = pd.DataFrame()\n\n        # Initialize the nextPageToken for pagination\n        nextPageToken = None\n\n        # Define the batch sizes for video fetching\n        record_fetching_batches = [50]  # Default batch size for YouTube API requests\n\n        # Adjust the batch sizes based on the max_record_count\n        if max_record_count > 50:\n            quotient = max_record_count // 50  # Number of full batches\n            remainder = [max_record_count % 50]  # Remaining records in the last batch\n            record_fetching_batches = record_fetching_batches * quotient\n            if remainder[0] > 0:\n                record_fetching_batches.extend(remainder)  # Add the remainder as a batch\n        else:\n            record_fetching_batches = [max_record_count]  # Single batch if max_record_count <= 50\n\n        # Case 1: Only one batch needed\n        if len(record_fetching_batches) == 1:\n            # Fetch video details for the single batch\n            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n            if response is None:\n                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n                return pd.DataFrame()\n\n            # Process the response into a DataFrame and get the nextPageToken\n            resultDataFrame, nextPageToken = VideoDataFrame(response)\n            nextPageToken = None  # Reset the token as no further pages are needed\n            if resultDataFrame is None:\n                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n                return pd.DataFrame()\n            return resultDataFrame\n\n        # Case 2: Multiple batches needed\n        elif len(record_fetching_batches) > 1:\n            # Fetch initial batch of video details\n            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n            if response is None:\n                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n                return pd.DataFrame()\n\n            # Process the response into a DataFrame and get the nextPageToken\n            resultDataFrame, nextPageToken = VideoDataFrame(response)\n            if resultDataFrame is None:\n                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n                return pd.DataFrame()\n\n            # Loop through subsequent batches\n            for batch in record_fetching_batches[1:]:\n                # Fetch details for the next batch using nextPageToken\n                response = VideoDetailExtractionNextPageToken(kw_list, nextPageToken, batch)\n                if response is None:\n                    print(\"Failed to fetch next page of video details - VideoDetailExtractionNextPageToken() returned None, hence returned till now fetched videoDetails.\")\n                    break\n\n                # Process the response into a DataFrame\n                resultDataFrame_next, nextPageToken = VideoDataFrame(response)\n                if resultDataFrame_next is not None:\n                    # Concatenate the new DataFrame to the result DataFrame\n                    resultDataFrame = pd.concat([resultDataFrame, resultDataFrame_next], ignore_index=True)\n\n                # Break the loop if we've reached the max record count or no more pages are available\n                if len(resultDataFrame) >= max_record_count or not nextPageToken:\n                    break\n\n        return resultDataFrame  # Return the final result DataFrame\n    except Exception as e:\n        print(f\"Error during VideoDetailsStructuring(), hence returned empty DataFrame: {e}\")\n        return pd.DataFrame()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.809539Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.809983Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.829764Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.809946Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.828707Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef RawFile(max_record_count):\n    \"\"\"\n    Processes video details, structures the data, and saves it as a JSON file.\n\n    Args:\n        max_record_count (int): The maximum number of records to process.\n\n    Returns:\n        bool: True if the file is successfully created and saved, False otherwise.\n    \"\"\"\n    try:\n        # Call the function to structure video details and return a DataFrame.\n        # `kw_list` is assumed to be a global variable containing the search keyword(s).\n        dataframe = VideoDetailsStructuring(max_record_count, kw_list)\n        \n        # Check if the DataFrame is not empty before saving.\n        if not dataframe.empty:\n            # Count the number of records (rows) in the DataFrame\n            record_count = len(dataframe)\n            \n            # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n            timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n        \n            # Create a filename using the generated timestamp to ensure uniqueness with number of records.\n            filename = f\"{timestamp}_{record_count}_records.json\"\n            \n            # Save the DataFrame to a JSON file with readable formatting.\n            dataframe.to_json(filename, orient=\"records\", indent=4)\n            print(f\"DataFrame saved as {filename}\")\n        else:\n            # Log a message if the DataFrame is empty.\n            print(\"No data to save since empty DataFrame returned.\")\n        \n        # Return True indicating the process was successful.\n        return True\n    except Exception as e:\n        # Handle and log any errors that occur during the process.\n        print(f\"Error during raw file creation: {e}\")\n        \n        # Return False indicating the process failed.\n        return False\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.831167Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.831549Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.854595Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.831509Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.853255Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef PushToGithub():\n    \"\"\"\n    Automates the process of identifying the latest .json file, copying it \n    to a GitHub repository, and pushing the changes.\n\n    Args:\n        None\n\n    Returns:\n        bool: True if the process completes successfully and the file is pushed to GitHub, \n              False if an error occurs during any step.\n    \"\"\"\n    # List all files in the working directory\n    output_files = os.listdir('/kaggle/working')\n    \n    try:\n        # Filter and find the most recent .json file\n        json_files = [file for file in output_files if file.endswith(\"records.json\")]\n        if json_files:\n            LatestFiles = max(json_files, key=os.path.getctime)  # Get the latest file based on creation time\n        else:\n            raise ValueError(\"No JSON files found!\")  # Raise an error if no JSON files are found\n    except ValueError as e:\n        print(f\"An error occurred at fetching recent .json file: {e}\")\n        return False  # Exit the function if there's an error in fetching JSON files\n    \n    # Define repository and destination paths\n    kaggle_repo_url = '/kaggle/working/YouTubeFoodChannelAnalysis'\n    destination_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Source/Daily'\n    \n    print(LatestFiles)  # Print the latest JSON file name\n    try:\n        # Check if the repository already exists\n        if os.path.exists(kaggle_repo_url):\n            print(\"Already cloned and the repo file exists\")\n            repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n            origin = repo.remote(name='origin')  # Get the remote repository\n            origin.pull()  # Pull the latest changes from the repository\n            print(\"Successfully pulled the git repo before push\")\n        else:\n            # Clone the repository if it doesn't exist\n            repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n            print(\"Successfully cloned the git repo\")\n        \n        # Check if the destination path exists, and copy the latest file\n        if os.path.exists(destination_path):\n            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n        else:\n            # Create the destination directory if it doesn't exist\n            os.makedirs(destination_path)\n            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n        \n        # Initialize the repository for git operations\n        repo = Repo(kaggle_repo_url)\n        # Add the copied file to the staging area\n        repo.index.add([f\"{destination_path}/{LatestFiles}\"])\n        \n        # Create a timestamp for the commit message\n        ist = datetime.timezone(datetime.timedelta(hours=5, minutes=30))  # IST timezone\n        timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n        # Commit the changes with a message including the timestamp and file name\n        repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {LatestFiles}\")\n        \n        # Push the changes to the remote repository\n        origin = repo.remote(name=\"origin\")\n        origin.push()\n        print(\"Output files successfully pushed to GitHub!\")\n        return True  # Return True if the process completes successfully\n    \n    except Exception as e:\n        # Handle any errors that occur during the git automation process\n        print(f\"An error occurred at git automation code: {e}\")\n        return False  # Return False if an error occurs\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.855592Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.856124Z\",\"iopub.status.idle\":\"2025-01-22T08:10:30.875431Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.856090Z\",\"shell.execute_reply\":\"2025-01-22T08:10:30.874212Z\"}}\ndef main(max_record_count):\n    \"\"\"\n    Main function to orchestrate the execution of raw data extraction and pushing data to GitHub.\n\n    Args:\n        max_record_count (int): The maximum number of records to process.\n    \"\"\"\n    # Call the RawFile function to process and extract raw data.\n    # This function likely handles fetching data, processing it, and storing it in a file.\n    RawFile(max_record_count)\n    \n    # Call the PushToGithub function to push the processed data to a GitHub repository.\n    # This function likely handles staging, committing, and pushing the file to the repository.\n    PushToGithub()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-22T08:10:30.876709Z\",\"iopub.execute_input\":\"2025-01-22T08:10:30.877202Z\",\"iopub.status.idle\":\"2025-01-22T08:10:43.813573Z\",\"shell.execute_reply.started\":\"2025-01-22T08:10:30.877156Z\",\"shell.execute_reply\":\"2025-01-22T08:10:43.812494Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Entry point of the script\nif __name__ == \"__main__\":\n    # Importing necessary libraries\n    from googleapiclient.discovery import build  # For interacting with YouTube API\n    from IPython.display import JSON, display  # For displaying JSON responses in Jupyter Notebooks\n    import re  # For regular expressions\n    import datetime  # For date and time manipulations\n    from dateutil.relativedelta import relativedelta  # For handling relative date differences\n    import pandas as pd  # For data manipulation and analysis\n    import os  # For interacting with the operating system\n    from kaggle_secrets import UserSecretsClient  # For securely managing API keys in Kaggle\n    import git  # For Git-related operations\n    from git import Repo  # For working with repositories\n    import shutil  # For file and directory operations\n    from pytz import timezone  # For handling time zones\n    from datetime import timedelta  # For handling time differences\n    \n    # Fetching secrets from Kaggle's secure environment\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")  # Fetch the YouTube API key\n    secret_value_1 = user_secrets.get_secret(\"repo_url_youtube_analysis\")  # Fetch the source repository URL\n    \n    # Assigning secrets to variables\n    api_key = secret_value_0\n    repo_url = secret_value_1\n    \n    # Setting up YouTube API details\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    youtube = build(api_service_name, api_version, developerKey=api_key)  # Initialize YouTube API client\n    \n    # Setting the timezone to Indian Standard Time (IST)\n    ist = timezone('Asia/Kolkata')\n    \n    # Maximum number of records to fetch\n    max_record_count = 4000\n    \n    # Keyword list for searching YouTube videos\n    kw_list = \"devops\"\n    \n    # Call the main function with the maximum record count as an argument\n    main(max_record_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:36.672068Z","iopub.execute_input":"2025-01-22T08:53:36.672402Z","iopub.status.idle":"2025-01-22T08:53:39.436085Z","shell.execute_reply.started":"2025-01-22T08:53:36.672376Z","shell.execute_reply":"2025-01-22T08:53:39.434925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Execution ended.\")\n\n# Record the end time of execution\nend_time = datetime.datetime.now()\n\n# Calculate the time taken for execution\ntime_taken = end_time - start_time\n\n# Get the current time in the 'Asia/Kolkata' timezone\ncurrent_time = datetime.datetime.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H:%M:%S\")\n\n# Create a new row as a DataFrame\nnew_row = pd.DataFrame([{\n    'ScriptFile': 'sourcedaily.ipynb',\n    'StartTime': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n    'EndTime': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n    'TimeTaken': str(time_taken),\n    'Date': current_time\n}])\n\n# Append the new row using pd.concat()\nFileExecution = pd.concat([FileExecution, new_row], ignore_index=True)\n\n# Display the DataFrame\n# display(FileExecution)\n\n# Save the DataFrame to a JSON file\nFileExecution.to_json(f\"{current_time}_ScriptFileExecution.json\", orient=\"records\", indent=4)\n# print(FileExecution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:39.437372Z","iopub.execute_input":"2025-01-22T08:53:39.437765Z","iopub.status.idle":"2025-01-22T08:53:39.458648Z","shell.execute_reply.started":"2025-01-22T08:53:39.437735Z","shell.execute_reply":"2025-01-22T08:53:39.457252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nAutomates the process of identifying the latest .json file, copying it \nto a GitHub repository, and pushing the changes.\n\nArgs:\n    None\n\nReturns:\n    bool: True if the process completes successfully and the file is pushed to GitHub, \n          False if an error occurs during any step.\n\"\"\"\n# List all files in the working directory\noutput_files = os.listdir('/kaggle/working')\n\ntry:\n    # Filter and find the most recent .json file\n    json_files = [file for file in output_files if file.endswith(\"ScriptFileExecution.json\")]\n    if json_files:\n        LatestFiles = max(json_files, key=os.path.getctime)  # Get the latest file based on creation time\n    else:\n        raise ValueError(\"No JSON files found!\")  # Raise an error if no JSON files are found\nexcept ValueError as e:\n    print(f\"An error occurred at fetching recent .json file: {e}\")\n\n# Define repository and destination paths\nkaggle_repo_url = '/kaggle/working/YouTubeFoodChannelAnalysis'\ndestination_path = '/kaggle/working/YouTubeFoodChannelAnalysis/ExecutionTracker/Daily'\n\nprint(LatestFiles)  # Print the latest JSON file name\ntry:\n    # Check if the repository already exists\n    if os.path.exists(kaggle_repo_url):\n        print(\"Already cloned and the repo file exists\")\n        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n        origin = repo.remote(name='origin')  # Get the remote repository\n        origin.pull()  # Pull the latest changes from the repository\n        print(\"Successfully pulled the git repo before push\")\n    else:\n        # Clone the repository if it doesn't exist\n        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n        print(\"Successfully cloned the git repo\")\n    \n    # Check if the destination path exists, and copy the latest file\n    if os.path.exists(destination_path):\n        shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n    else:\n        # Create the destination directory if it doesn't exist\n        os.makedirs(destination_path)\n        shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n    \n    # Initialize the repository for git operations\n    repo = Repo(kaggle_repo_url)\n    # Add the copied file to the staging area\n    repo.index.add([f\"{destination_path}/{LatestFiles}\"])\n    \n    # Create a timestamp for the commit message\n    ist = datetime.timezone(datetime.timedelta(hours=5, minutes=30))  # IST timezone\n    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n    # Commit the changes with a message including the timestamp and file name\n    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {LatestFiles}\")\n    \n    # Push the changes to the remote repository\n    origin = repo.remote(name=\"origin\")\n    origin.push()\n    print(\"Execution Tracking file successfully pushed to GitHub!\")\n\nexcept Exception as e:\n    # Handle any errors that occur during the git automation process\n    print(f\"An error occurred at git automation code: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:39.607650Z","iopub.execute_input":"2025-01-22T08:53:39.608059Z","iopub.status.idle":"2025-01-22T08:53:41.632619Z","shell.execute_reply.started":"2025-01-22T08:53:39.608026Z","shell.execute_reply":"2025-01-22T08:53:41.631316Z"}},"outputs":[],"execution_count":null}]}