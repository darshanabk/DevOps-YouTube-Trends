{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a28c05",
   "metadata": {
    "_cell_guid": "8880b24c-1a3c-449b-a134-cc8c95a707c9",
    "_uuid": "bdf6f78f-f711-4e13-954f-7d4f9ddf327f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004927,
     "end_time": "2025-02-04T22:26:19.311843",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.306916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Observation:**  \n",
    "\n",
    "1. Null values are present in the following columns:\n",
    "   - **`videoDefaultLanguage`**  (will be dropped after data cleaning)\n",
    "   - **`videoDefaultAudioLanguage`** \n",
    "   - **`channelCountry`**\n",
    "\n",
    "---\n",
    "\n",
    "2. The following columns will be dropped as part of data cleaning:\n",
    "   - **`videoDescription`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoLiveBroadcastContent`**: All values are `'none'`, providing no variability or insights. \n",
    "   - **`videoFavoriteCount`**: All values are `0`, making it redundant.  \n",
    "   - **`videoTags`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoUniqueId`**: Identified as a duplicate column.  \n",
    "   - **`channelIdUnique`**: Identified as a duplicate column.  \n",
    "   - **`channelTitleCheck`**: Identified as a duplicate column.  \n",
    "   - **`channelDescription`**: Reserved for analysis in future NLP project with a broader dataset.\n",
    "---\n",
    "\n",
    "3. The columns **`channelName`** and **`videoTitle`** require further processing due to the presence of:\n",
    "    - Multilingual text.  \n",
    "    - Emojis and special characters.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1d079f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.321840Z",
     "iopub.status.busy": "2025-02-04T22:26:19.321333Z",
     "iopub.status.idle": "2025-02-04T22:26:19.329178Z",
     "shell.execute_reply": "2025-02-04T22:26:19.328147Z"
    },
    "papermill": {
     "duration": 0.014914,
     "end_time": "2025-02-04T22:26:19.331138",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.316224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Source_File_Extraction(repo_url, kaggle_repo_url, source_path):\n",
    "    \"\"\"\n",
    "    This function checks if a specified Git repository already exists in the local system.\n",
    "    If the repository exists, it pulls the latest changes from the remote repository.\n",
    "    If the repository doesn't exist, it clones the repository from the provided URL.\n",
    "    \n",
    "    After ensuring the repository is up-to-date, it searches for a JSON file that starts with \"S_\" \n",
    "    and ends with \"records.json\" in the specified source directory, loads the file using pandas, \n",
    "    and returns the data as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): The URL of the Git repository to clone if not already present.\n",
    "    - kaggle_repo_url (str): The local path where the repository is stored or will be cloned to.\n",
    "    - source_path (str): The directory where the JSON file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The data from the JSON file as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the source path and find the relevant JSON file\n",
    "    output_files = os.listdir(source_path)\n",
    "    Source_File = max([i for i in output_files if i.startswith(\"S_\") and i.endswith('records.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Source_File = pd.read_json(f'{source_path}/{Source_File}')\n",
    "    \n",
    "    return Source_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4599ba7e",
   "metadata": {
    "_cell_guid": "fb4272e5-8031-4140-8ddf-7df2566e062e",
    "_uuid": "53f3d728-5d7f-41f7-9a62-857c802989f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.341453Z",
     "iopub.status.busy": "2025-02-04T22:26:19.341092Z",
     "iopub.status.idle": "2025-02-04T22:26:19.347503Z",
     "shell.execute_reply": "2025-02-04T22:26:19.346310Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01354,
     "end_time": "2025-02-04T22:26:19.349361",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.335821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path):\n",
    "    \"\"\"\n",
    "    Ensures the repository is up-to-date by either pulling the latest changes or cloning it.\n",
    "    Then, extracts and returns the most recent JSON file starting with \"RE_\" and ending with \n",
    "    \"country_details.json\" from the specified requirement directory as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): Git repository URL to clone if not present.\n",
    "    - kaggle_repo_url (str): Local directory path for the repository.\n",
    "    - requirement_path (str): Directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Data from the most recent JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        \n",
    "        # Access the existing repository and pull the latest changes\n",
    "        repo = git.Repo(kaggle_repo_url)\n",
    "        origin = repo.remote(name='origin')\n",
    "        origin.pull()  # Pull the latest changes\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist locally\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the requirement directory\n",
    "    output_files = os.listdir(requirement_path)\n",
    "    \n",
    "    # Find the most recent JSON file that starts with \"RE_\" and ends with \"country_details.json\"\n",
    "    Requirement_File = max([i for i in output_files if i.startswith(\"RE_\") and i.endswith('country_details.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Requirement_File = pd.read_json(f'{requirement_path}/{Requirement_File}')\n",
    "    \n",
    "    return Requirement_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637f52df",
   "metadata": {
    "_cell_guid": "0c40861f-eff0-4427-9093-490bdb39fd91",
    "_uuid": "90b8ff78-fa6c-42a9-90a6-ecb894e75e6c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.359305Z",
     "iopub.status.busy": "2025-02-04T22:26:19.358950Z",
     "iopub.status.idle": "2025-02-04T22:26:19.368579Z",
     "shell.execute_reply": "2025-02-04T22:26:19.367564Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016732,
     "end_time": "2025-02-04T22:26:19.370438",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.353706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataCleaning(Target_File):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by performing the following operations:\n",
    "    1. Drops irrelevant columns.\n",
    "    2. Removes duplicate rows.\n",
    "    3. Filters videos based on language (only those with 'videoDefaultAudioLanguage' starting with 'en').\n",
    "    4. Translates non-ASCII characters in 'channelName' and 'videoTitle' to English.\n",
    "    5. Removes emojis and decodes HTML entities from 'channelName' and 'videoTitle'.\n",
    "    6. Removes non-ASCII characters from 'channelName' and 'videoTitle'.\n",
    "    7. Fills missing values in 'channelCountry' with 'Unknown'.\n",
    "    8. Returns the cleaned DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - Target_File (pd.DataFrame): The DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    Target_File = Target_File.drop(['videoDescription', 'videoLiveBroadcastContent', 'videoFavoriteCount',\n",
    "                                    'videoTags', 'videoUniqueId', 'channelIdUnique', 'channelTitleCheck', 'channelDescription'], axis=1)\n",
    "    \n",
    "    # Identify and keep all duplicates\n",
    "    duplicates = Target_File[Target_File.duplicated(keep=False)]  # Selects all duplicates, including the first occurrence\n",
    "    \n",
    "    # Remove duplicates\n",
    "    Target_File = Target_File.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Filter for videos with 'videoDefaultAudioLanguage' starting with 'en'\n",
    "    Target_File_EN = Target_File[Target_File['videoDefaultAudioLanguage'].str.startswith(\"en\", na=False)].reset_index(drop=True)\n",
    "\n",
    "    # Iterate through each row in 'Target_File_EN' to clean 'channelName' and 'videoTitle'\n",
    "    for i in range(len(Target_File_EN['channelName'])):\n",
    "        try:\n",
    "            # Translate non-ASCII characters in 'channelName' and 'videoTitle' to English\n",
    "            if not Target_File_EN['channelName'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'channelName'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['channelName'][i])\n",
    "            if not Target_File_EN['videoTitle'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'videoTitle'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['videoTitle'][i])\n",
    "\n",
    "            # Remove emojis\n",
    "            Target_File_EN.loc[i, 'channelName'] = emoji.replace_emoji(Target_File_EN['channelName'][i], replace='')\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = emoji.replace_emoji(Target_File_EN['videoTitle'][i], replace='')\n",
    "\n",
    "            # Decode HTML entities like &amp; and &#39;\n",
    "            Target_File_EN.loc[i, 'channelName'] = html.unescape(Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = html.unescape(Target_File_EN['videoTitle'][i])\n",
    "\n",
    "            # Remove non-ASCII characters from 'channelName' and 'videoTitle'\n",
    "            Target_File_EN.loc[i, 'channelName'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['videoTitle'][i])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Remove duplicates after the transformations\n",
    "    Target_File_EN = Target_File_EN.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Drop 'videoDefaultLanguage' column as it is no longer needed\n",
    "    Target_File_EN = Target_File_EN.drop(['videoDefaultLanguage'], axis=1)\n",
    "    \n",
    "    # Fill missing values in 'channelCountry' with 'Unknown'\n",
    "    Target_File_EN['channelCountry'] = Target_File_EN['channelCountry'].fillna('Unknown')\n",
    "    \n",
    "    return Target_File_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b387a",
   "metadata": {
    "_cell_guid": "209556f7-89f1-433e-903f-f18db03bd537",
    "_uuid": "1f886a68-6963-4e9d-9746-955583355c13",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.380371Z",
     "iopub.status.busy": "2025-02-04T22:26:19.380011Z",
     "iopub.status.idle": "2025-02-04T22:26:19.385314Z",
     "shell.execute_reply": "2025-02-04T22:26:19.384411Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012056,
     "end_time": "2025-02-04T22:26:19.386882",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.374826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def videoDurationClassification(videoDurationInSeconds):\n",
    "    \"\"\"\n",
    "    Classifies the video duration into categories based on its length in seconds.\n",
    "\n",
    "    Args:\n",
    "    - videoDurationInSeconds (int): Duration of the video in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string indicating the classification of the video duration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Classifying the video duration into different categories\n",
    "    if 0 <= videoDurationInSeconds <= 60:\n",
    "        return \"Very Short\"  # Video duration between 0 and 60 seconds\n",
    "    elif 61 <= videoDurationInSeconds <= 120:\n",
    "        return \"Short\"  # Video duration between 61 and 120 seconds\n",
    "    elif 121 <= videoDurationInSeconds <= 300:\n",
    "        return \"Medium\"  # Video duration between 121 and 300 seconds (2-5 minutes)\n",
    "    elif 301 <= videoDurationInSeconds <= 600:\n",
    "        return \"Long\"  # Video duration between 301 and 600 seconds (5-10 minutes)\n",
    "    elif 601 <= videoDurationInSeconds <= 3600:\n",
    "        return \"Very Long\"  # Video duration between 601 seconds (10 minutes) and 3600 seconds (1 hour)\n",
    "    elif 3601 <= videoDurationInSeconds <= 10800:\n",
    "        return \"Extended\"  # Video duration between 3601 seconds (1 hour) and 10800 seconds (3 hours)\n",
    "    elif videoDurationInSeconds > 10800:\n",
    "        return \"Ultra Long\"  # Video duration greater than 10800 seconds (3 hours)\n",
    "    else:\n",
    "        return \"Invalid video duration\"  # Invalid value for video duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe2c1",
   "metadata": {
    "_cell_guid": "01ee4c25-82cb-4423-8e7a-56f60b0814d3",
    "_uuid": "9b7341a5-8309-4828-af31-53ccfa3c6e23",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003968,
     "end_time": "2025-02-04T22:26:19.395186",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.391218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Channel Growth Calculation\n",
    "\n",
    "## **Overview**\n",
    "The channel growth metric is designed to assess the growth of a YouTube channel based on key engagement indicators: \n",
    "- **Channel View Count**\n",
    "- **Channel Subscriber Count**\n",
    "- **Channel Video Count**\n",
    "- **Channel Age (in years)**\n",
    "\n",
    "## **Formula**\n",
    "The channel growth score is computed as:\n",
    "\n",
    "```python\n",
    "channel_growth = ((normalized_views * weight_views) + \n",
    "                  (normalized_subscribers * weight_subscribers) + \n",
    "                  (normalized_videos * weight_videos)) / channel_age_in_years\n",
    "```\n",
    "\n",
    "where:\n",
    "- **Min-Max Normalization** is applied to views, subscribers, and video count:\n",
    "  ```python\n",
    "  normalized_value = (value - min_value) / (max_value - min_value)\n",
    "  ```\n",
    "- **Weighting factors** determine the relative importance of each metric:\n",
    "  - `weight_views = 0.5`\n",
    "  - `weight_subscribers = 0.3`\n",
    "  - `weight_videos = 0.2`\n",
    "- **Channel Age (years)** is computed from:\n",
    "  ```python\n",
    "  channel_age_in_years = (current_timestamp - channelPublishedOnInSeconds) / (365 * 24 * 60 * 60)\n",
    "  ```\n",
    "\n",
    "## **Concepts Used**\n",
    "1. **Min-Max Normalization**: Scales values between 0 and 1.\n",
    "   - [Feature Scaling (Wikipedia)](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "2. **Weighted Scoring**: Prioritizes key metrics based on impact.\n",
    "   - [Weighted Scoring Model](https://theproductmanager.com/topics/weighted-scoring-model/)\n",
    "3. **YouTube Analytics Metrics**: Defines the importance of views, subscribers, and videos.\n",
    "   - [YouTube Analytics Help](https://support.google.com/youtube/answer/9002587?hl=en)\n",
    "4. **Channel Age Calculation**: Determines the time span since the channel was created.\n",
    "   - [Unix Time Conversion](https://www.unixtimestamp.com/)\n",
    "\n",
    "## **Implementation Example**\n",
    "```python\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def calculate_channel_growth(view_count, subscriber_count, video_count, channel_published_on):\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone)\n",
    "    channel_age_in_years = (current_ist_time - channel_published_on) / (365 * 24 * 60 * 60)\n",
    "    \n",
    "    # Min-Max normalization (Assume predefined min/max values from dataset)\n",
    "    min_views, max_views = 1000, 10000000\n",
    "    min_subscribers, max_subscribers = 10, 1000000\n",
    "    min_videos, max_videos = 1, 10000\n",
    "    \n",
    "    normalized_views = (view_count - min_views) / (max_views - min_views)\n",
    "    normalized_subscribers = (subscriber_count - min_subscribers) / (max_subscribers - min_subscribers)\n",
    "    normalized_videos = (video_count - min_videos) / (max_videos - min_videos)\n",
    "    \n",
    "    # Weighted sum\n",
    "    growth_score = ((normalized_views * 0.5) + (normalized_subscribers * 0.3) + (normalized_videos * 0.2)) / channel_age_in_years\n",
    "    \n",
    "    return growth_score\n",
    "```\n",
    "\n",
    "## **Conclusion**\n",
    "This approach helps in analyzing a YouTube channel's growth potential by factoring in **engagement, longevity, and content volume**. It provides a scalable and adaptable framework for evaluating growth trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b201ed",
   "metadata": {
    "_cell_guid": "e73a0ab7-6841-4b2d-bd2d-0a69b0fa79cb",
    "_uuid": "3111d446-93da-4b3f-b1dd-6b8f77b17657",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.405194Z",
     "iopub.status.busy": "2025-02-04T22:26:19.404835Z",
     "iopub.status.idle": "2025-02-04T22:26:19.409528Z",
     "shell.execute_reply": "2025-02-04T22:26:19.408529Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011484,
     "end_time": "2025-02-04T22:26:19.411208",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.399724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(series):\n",
    "    \"\"\"\n",
    "    Applies Min-Max normalization to a Pandas Series, scaling the values to a range between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "    - series (pd.Series): The input data series to normalize.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: A normalized series with values scaled between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Min-Max normalization formula: (x - min) / (max - min)\n",
    "    return (series - series.min()) / (series.max() - series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ee95ce",
   "metadata": {
    "_cell_guid": "e03be790-00ba-4ad2-85bb-56dc95fa7826",
    "_uuid": "c1295e86-6abe-491b-8100-2ecc8055a8f4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.421205Z",
     "iopub.status.busy": "2025-02-04T22:26:19.420770Z",
     "iopub.status.idle": "2025-02-04T22:26:19.426018Z",
     "shell.execute_reply": "2025-02-04T22:26:19.424920Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012197,
     "end_time": "2025-02-04T22:26:19.427783",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.415586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_datetime(value):\n",
    "    \"\"\"\n",
    "    Parses a datetime string into a Pandas datetime object based on specific formats.\n",
    "    \n",
    "    Handles two datetime formats:\n",
    "    1. \"%Y-%m-%dT%H:%M:%SZ\" for ISO 8601 format without fractional seconds.\n",
    "    2. \"%Y-%m-%dT%H:%M:%S.%fZ\" for ISO 8601 format with fractional seconds.\n",
    "\n",
    "    Args:\n",
    "    - value (str): The input datetime string to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Timestamp or pd.NaT: A Pandas Timestamp object if the format matches, otherwise pd.NaT.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for the presence of \"Z\" and determine the format based on whether the string contains a decimal point\n",
    "    if \"Z\" in value and \".\" not in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%SZ\")  # Format without fractional seconds\n",
    "    elif \"Z\" in value and \".\" in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%S.%fZ\")  # Format with fractional seconds\n",
    "    else:\n",
    "        return pd.NaT  # Return Not a Time (NaT) if the format doesn't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b6e7b4",
   "metadata": {
    "_cell_guid": "9569775c-e346-431e-a079-e3a3eb79c309",
    "_uuid": "ebf1cf65-237b-402b-84b0-21a57d4c1339",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.437852Z",
     "iopub.status.busy": "2025-02-04T22:26:19.437481Z",
     "iopub.status.idle": "2025-02-04T22:26:19.445408Z",
     "shell.execute_reply": "2025-02-04T22:26:19.444481Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014799,
     "end_time": "2025-02-04T22:26:19.446923",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.432124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_channel_growth(Cleaned_File):\n",
    "    \"\"\"\n",
    "    Calculates the growth score for channels and engagement score for videos based on various metrics.\n",
    "    \n",
    "    The growth score for each channel is calculated using the normalized values of:\n",
    "    - View count\n",
    "    - Subscriber count\n",
    "    - Video count\n",
    "    - Channel age (in years)\n",
    "    \n",
    "    The engagement score for each video is calculated using:\n",
    "    - Views per day\n",
    "    - Like-to-view ratio\n",
    "    - Comment-to-view ratio\n",
    "\n",
    "    Args:\n",
    "    - Cleaned_File (pd.DataFrame): The dataframe containing channel and video data to calculate growth and engagement scores.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned dataframe with the added columns for growth and engagement scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the current IST time (Indian Standard Time) to calculate age-based metrics\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone).replace(tzinfo=None) \n",
    "    \n",
    "    # Channel Age Calculation (in years)\n",
    "    channelPublishedOn = Cleaned_File[\"channelPublishedOn\"].apply(parse_datetime)\n",
    "    Cleaned_File['channelAgeInYears'] = (current_ist_time - channelPublishedOn).dt.total_seconds() / (365 * 24 * 60 * 60)\n",
    "    \n",
    "    # Min-Max normalization for channel metrics\n",
    "    Cleaned_File['channelNormalizedViewCount'] = normalize(Cleaned_File['channelViewCount'])\n",
    "    Cleaned_File['channelNormalizedSubscriberCount'] = normalize(Cleaned_File['channelSubscriberCount'])\n",
    "    Cleaned_File['channelNormalizedVideoCount'] = normalize(Cleaned_File['channelVideoCount'])\n",
    "    Cleaned_File['channelNormalizedChannelAge'] = normalize(Cleaned_File['channelAgeInYears'])\n",
    "    \n",
    "    # Define weights for each metric in the growth score calculation\n",
    "    weight_views = 0.5\n",
    "    weight_subscribers = 0.3\n",
    "    weight_videos = 0.2\n",
    "    \n",
    "    # Growth Score Calculation for the channel\n",
    "    Cleaned_File['channelGrowthScore'] = (\n",
    "        (Cleaned_File['channelNormalizedViewCount'] * weight_views) +\n",
    "        (Cleaned_File['channelNormalizedSubscriberCount'] * weight_subscribers) +\n",
    "        (Cleaned_File['channelNormalizedVideoCount'] * weight_videos)\n",
    "    ) / Cleaned_File['channelNormalizedChannelAge']\n",
    "    \n",
    "    # Video Age Calculation (in days)\n",
    "    videoPublishedOn = Cleaned_File[\"videoPublishedOn\"].apply(parse_datetime)\n",
    "    Cleaned_File[\"videoAgeInDays\"] = (current_ist_time - videoPublishedOn).dt.total_seconds() / (24 * 60 * 60)\n",
    "    \n",
    "    # Engagement Metrics for videos\n",
    "    Cleaned_File[\"videoViewsPerDay\"] = Cleaned_File[\"videoViewCount\"] / Cleaned_File[\"videoAgeInDays\"]\n",
    "    Cleaned_File[\"videoLikeToViewRatio\"] = Cleaned_File[\"videoLikeCount\"] / Cleaned_File[\"videoViewCount\"]\n",
    "    Cleaned_File[\"videoCommentToViewRatio\"] = Cleaned_File[\"videoCommentCount\"] / Cleaned_File[\"videoViewCount\"]\n",
    "    \n",
    "    # Engagement Score Calculation for the video\n",
    "    Cleaned_File[\"videoEngagementScore\"] = (\n",
    "        (Cleaned_File[\"videoViewsPerDay\"] * 0.5) +\n",
    "        (Cleaned_File[\"videoLikeToViewRatio\"] * 100 * 0.3) +\n",
    "        (Cleaned_File[\"videoCommentToViewRatio\"] * 100 * 0.2)\n",
    "    )\n",
    "    \n",
    "    # Return the dataframe with added growth and engagement scores\n",
    "    return Cleaned_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62ffdd9",
   "metadata": {
    "_cell_guid": "5d36dd27-6efd-4cec-a420-3fd081f2dcd0",
    "_uuid": "3ae75ac8-d426-405b-a278-0a72b89266c9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.456749Z",
     "iopub.status.busy": "2025-02-04T22:26:19.456382Z",
     "iopub.status.idle": "2025-02-04T22:26:19.462998Z",
     "shell.execute_reply": "2025-02-04T22:26:19.461949Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013499,
     "end_time": "2025-02-04T22:26:19.464701",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.451202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FeatureEngineering(Cleaned_File):\n",
    "    \"\"\"\n",
    "    This function performs feature engineering to enhance the dataset for analysis by creating new features \n",
    "    and transforming existing ones, such as categorizing video duration, calculating channel growth and \n",
    "    video engagement scores, and enriching geographic details.\n",
    "\n",
    "    The key steps include:\n",
    "    - Extracting the day of the week from the video publish timestamp.\n",
    "    - Classifying video durations into predefined categories.\n",
    "    - Calculating channel growth and video engagement scores.\n",
    "    - Ranking channels and videos based on their growth and engagement scores.\n",
    "    - Merging geographic details like country, continent, and IT hub information with the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - Cleaned_File (pd.DataFrame): The input dataframe containing video and channel data for feature engineering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The transformed dataframe with newly engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feature: videoPublishedWeekDay - Derive the day of the week from the videoPublishedOn timestamp.\n",
    "    Cleaned_File['videoPublishedWeekDay'] = pd.to_datetime(Cleaned_File[\"videoPublishedOn\"]).dt.day_name()\n",
    "    \n",
    "    # Feature: videoDurationClassification - Categorize videos based on their duration in seconds into predefined segments.\n",
    "    # Categories:\n",
    "    #     Very Short (0 - 60 sec), Short (61 sec - 2 min), Medium (2 min 1 sec - 5 min),\n",
    "    #     Long (5 min 1 sec - 10 min), Very Long (10 min 1 sec - 1 hour),\n",
    "    #     Extended (1 hour 1 sec - 3 hours), Ultra Long (3 hours 1 sec and above)\n",
    "    Cleaned_File['videoDurationClassification'] = Cleaned_File['videoDurationInSeconds'].apply(videoDurationClassification)\n",
    "    \n",
    "    # Feature: channelGrowth metric - Calculate channel growth using factors such as views, subscribers, video count, and age.\n",
    "    # Normalization of key columns: channelPublishedOn, channelViewCount, channelSubscriberCount, and channelVideoCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)\n",
    "    \n",
    "    # Feature: videoEngagementScore - Calculate the video engagement score using video views, likes, and comments.\n",
    "    # Normalization of key columns: videoPublishedOn, videoViewCount, videoLikeCount, and videoCommentCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)  # This also handles the video engagement scores\n",
    "    \n",
    "    # Feature: channelGrowthScoreRank - Rank channels based on their growth score.\n",
    "    Cleaned_File[\"channelGrowthScoreRank\"] = Cleaned_File[\"channelGrowthScore\"].rank()\n",
    "    \n",
    "    # Feature: videoEngagementScoreRank - Rank videos based on their engagement score.\n",
    "    Cleaned_File[\"videoEngagementScoreRank\"] = Cleaned_File[\"videoEngagementScore\"].rank()\n",
    "    \n",
    "    # Feature: Geographic Classification - Enrich dataset with geographic details (country, continent, IT hub classification).\n",
    "    # This merges additional country and continent details from an external source based on the channel's country.\n",
    "    \n",
    "    # Fetch geographic details (ISO codes, country names, continent, etc.) from an external file\n",
    "    Country_Details_ISO = Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path).transpose()\n",
    "    Country_Details_ISO = Country_Details_ISO.reset_index()\n",
    "    Country_Details_ISO.rename(columns={'index': 'country_code'}, inplace=True)\n",
    "    \n",
    "    # Merge geographic details (from Country_Details_ISO) with the cleaned file\n",
    "    resultDataFrame = pd.merge(Cleaned_File, Country_Details_ISO, left_on='channelCountry', right_on='country_code', how='left')\n",
    "    \n",
    "    # Fill missing geographic data with 'Unknown' (in case a country code doesn't match)\n",
    "    resultDataFrame.fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Return the enriched dataframe with new features\n",
    "    return resultDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bbeeb3",
   "metadata": {
    "_cell_guid": "205a6af6-8466-4dd9-b93c-c449161746c1",
    "_uuid": "e516fe25-6419-4897-a2f0-e818417ff145",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.474395Z",
     "iopub.status.busy": "2025-02-04T22:26:19.474051Z",
     "iopub.status.idle": "2025-02-04T22:26:19.481101Z",
     "shell.execute_reply": "2025-02-04T22:26:19.480167Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013603,
     "end_time": "2025-02-04T22:26:19.482648",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.469045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GitHubPush(Target_File_EN):\n",
    "    \"\"\"\n",
    "    This function handles the process of saving a cleaned and processed DataFrame as a JSON file, \n",
    "    pushing it to a GitHub repository. It ensures that the file is properly named with a timestamp \n",
    "    and number of records, creates necessary directories, and commits the changes to the repository.\n",
    "    \n",
    "    Args:\n",
    "    - Target_File_EN (pd.DataFrame): The DataFrame that contains the processed data to be saved and pushed.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function performs file handling and Git operations but does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of records in the DataFrame\n",
    "    record_count = len(Target_File_EN)\n",
    "    \n",
    "    # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    \n",
    "    # Create a filename using the generated timestamp and number of records to ensure uniqueness.\n",
    "    filename = f\"DC_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame to a JSON file in a readable format (with indentation)\n",
    "    Target_File_EN.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    # Check if the destination directory exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(destination_path)\n",
    "        print('Created the destination directory, DataCleaning/Daily')\n",
    "        # Copy the saved file into the newly created directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    else:\n",
    "        print('Destination directory already exists')\n",
    "        # Copy the file to the existing directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the repository for git operations using the local GitHub repository URL\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the staging area for git commit\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a timestamp for the commit message\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    # Commit the changes with a message that includes the timestamp and the filename\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the changes to the remote repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    push_result = origin.push()\n",
    "    \n",
    "    # Check if the push was successful and print the result\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab2f9ba",
   "metadata": {
    "_cell_guid": "7d1f0160-960d-497b-b526-af715dd51b8f",
    "_uuid": "28063bdc-f8f9-40b6-8c3b-a7c2f2bf08e6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.492326Z",
     "iopub.status.busy": "2025-02-04T22:26:19.491978Z",
     "iopub.status.idle": "2025-02-04T22:26:19.496761Z",
     "shell.execute_reply": "2025-02-04T22:26:19.495751Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011442,
     "end_time": "2025-02-04T22:26:19.498449",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.487007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function orchestrates the entire data pipeline by:\n",
    "    1. Extracting the source data from the given repository URL.\n",
    "    2. Cleaning the extracted data using the DataCleaning function.\n",
    "    3. Applying feature engineering to the cleaned data using the FeatureEngineering function.\n",
    "    4. Pushing the final processed file to a GitHub repository.\n",
    "    \n",
    "    This function executes the steps in sequence to process and upload data.\n",
    "    \n",
    "    Args:\n",
    "    - None: This function does not accept any arguments. It uses predefined repository URLs and paths.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function does not return anything but performs data processing and Git operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract the source file from the repository based on the provided URL and path.\n",
    "    Source_File = Source_File_Extraction(repo_url, kaggle_repo_url, source_path)\n",
    "    \n",
    "    # Step 2: Clean the extracted data using the DataCleaning function.\n",
    "    Cleaned_File = DataCleaning(Source_File)\n",
    "    \n",
    "    # Optional: Uncomment to display the cleaned file sorted by video duration.\n",
    "    # display(Cleaned_File.sort_values(by='videoDurationInSeconds', ascending=True))\n",
    "    \n",
    "    # Step 3: Apply feature engineering to the cleaned data.\n",
    "    Feature_File = FeatureEngineering(Cleaned_File)\n",
    "    \n",
    "    # Optional: Uncomment to display the feature-engineered file.\n",
    "    # display(Feature_File)\n",
    "    \n",
    "    # Step 4: Push the processed and feature-engineered data to GitHub using GitHubPush function.\n",
    "    GitHubPush(Feature_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "114030e4",
   "metadata": {
    "_cell_guid": "d76dd393-e328-4344-b0ff-28872213af8b",
    "_uuid": "952fcae8-b788-44b3-882f-95442928042a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T22:26:19.508534Z",
     "iopub.status.busy": "2025-02-04T22:26:19.508146Z",
     "iopub.status.idle": "2025-02-04T22:26:29.393008Z",
     "shell.execute_reply": "2025-02-04T22:26:29.391682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.891903,
     "end_time": "2025-02-04T22:26:29.394870",
     "exception": false,
     "start_time": "2025-02-04T22:26:19.502967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned the git repo\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "DataFrame saved as DC_2025-02-05_03:56:28_412_records.json\n",
      "Destination directory already exists\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    This script is the entry point for the data cleaning and feature engineering pipeline.\n",
    "    It performs the following tasks:\n",
    "    1. Imports necessary libraries for data processing, file handling, and Git operations.\n",
    "    2. Retrieves user secrets for repository URL.\n",
    "    3. Sets up paths for different directories (source, destination, etc.).\n",
    "    4. Configures pandas to display all columns and rows without truncation.\n",
    "    5. Calls the main function to execute the pipeline.\n",
    "\n",
    "    The script is designed to be executed as the main module in a Python environment.\n",
    "    It ensures that all necessary operations are performed, including fetching source data, \n",
    "    cleaning, feature engineering, and pushing the final data to a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import os  \n",
    "    import git  # Git library for interacting with repositories\n",
    "    from git import Repo  # GitHub repository interaction\n",
    "    import time  # For time-related operations\n",
    "    import datetime  # For working with date and time\n",
    "    from pytz import timezone  # For timezone management\n",
    "    import pytz  # Timezone handling\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import deep_translator  # For translation services\n",
    "    from deep_translator import GoogleTranslator  # Google Translate API integration\n",
    "    import shutil  # For file operations like copying or removing\n",
    "    import emoji  # For handling emojis in the data\n",
    "    import re  # For regular expression operations\n",
    "    import html  # For HTML parsing and escaping\n",
    "    from kaggle_secrets import UserSecretsClient  # For accessing Kaggle's secret management system\n",
    "    \n",
    "    # Retrieve secret value for repository URL from Kaggle secrets storage\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"dataCleanRepoUrl\")\n",
    "    repo_url = secret_value_0  # URL for the GitHub repository used in this pipeline\n",
    "    \n",
    "    # Set timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Define paths for different directories\n",
    "    kaggle_repo_url = '/kaggle/working/YouTubeFoodChannelAnalysis'  # Path to the working repository on Kaggle\n",
    "    destination_path = '/kaggle/working/YouTubeFoodChannelAnalysis/DataCleaning/Daily'  # Path to store cleaned data\n",
    "    source_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Source/Daily'  # Path to source raw data\n",
    "    requirement_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Requirement/Daily'  # Path to requirement files\n",
    "    \n",
    "    # Configure pandas to display all columns and rows without truncation for easier debugging\n",
    "    pd.set_option(\"display.max_columns\", None)  # Prevent truncating columns\n",
    "    pd.set_option(\"display.max_rows\", None)  # Prevent truncating rows\n",
    "    \n",
    "    # Call the main function to execute the data pipeline\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.518102,
   "end_time": "2025-02-04T22:26:30.019717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-04T22:26:16.501615",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
