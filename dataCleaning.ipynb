{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2b6ec7",
   "metadata": {
    "_cell_guid": "77403444-565c-4629-8e75-963d39346ad0",
    "_uuid": "d21d04f3-0df2-4b2e-a435-a0f1812db540",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004458,
     "end_time": "2025-02-04T20:03:27.476922",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.472464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Observation:**  \n",
    "\n",
    "1. Null values are present in the following columns:\n",
    "   - **`videoDefaultLanguage`**  (will be dropped after data cleaning)\n",
    "   - **`videoDefaultAudioLanguage`** \n",
    "   - **`channelCountry`**\n",
    "\n",
    "---\n",
    "\n",
    "2. The following columns will be dropped as part of data cleaning:\n",
    "   - **`videoDescription`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoLiveBroadcastContent`**: All values are `'none'`, providing no variability or insights. \n",
    "   - **`videoFavoriteCount`**: All values are `0`, making it redundant.  \n",
    "   - **`videoTags`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoUniqueId`**: Identified as a duplicate column.  \n",
    "   - **`channelIdUnique`**: Identified as a duplicate column.  \n",
    "   - **`channelTitleCheck`**: Identified as a duplicate column.  \n",
    "   - **`channelDescription`**: Reserved for analysis in future NLP project with a broader dataset.\n",
    "---\n",
    "\n",
    "3. The columns **`channelName`** and **`videoTitle`** require further processing due to the presence of:\n",
    "    - Multilingual text.  \n",
    "    - Emojis and special characters.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff954d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.486877Z",
     "iopub.status.busy": "2025-02-04T20:03:27.486479Z",
     "iopub.status.idle": "2025-02-04T20:03:27.494253Z",
     "shell.execute_reply": "2025-02-04T20:03:27.493163Z"
    },
    "papermill": {
     "duration": 0.014571,
     "end_time": "2025-02-04T20:03:27.496063",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.481492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Source_File_Extraction(repo_url,kaggle_repo_url):\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    output_files = os.listdir(source_path)\n",
    "    Source_File = max([i for i in output_files if i.startswith(\"S_\") and i.endswith('records.json')])\n",
    "    Source_File = pd.read_json(f'{source_path}/{Source_File}')\n",
    "    return Source_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7558cb26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.505087Z",
     "iopub.status.busy": "2025-02-04T20:03:27.504755Z",
     "iopub.status.idle": "2025-02-04T20:03:27.510801Z",
     "shell.execute_reply": "2025-02-04T20:03:27.509560Z"
    },
    "papermill": {
     "duration": 0.012265,
     "end_time": "2025-02-04T20:03:27.512470",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.500205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Requirement_File_Extraction(repo_url,kaggle_repo_url):\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    output_files = os.listdir(requirement_path)\n",
    "    Requirement_File = max([i for i in output_files if i.startswith(\"RE_\") and i.endswith('country_details.json')])\n",
    "    Requirement_File = pd.read_json(f'{requirement_path}/{Requirement_File}')\n",
    "    return Requirement_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d45ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.521410Z",
     "iopub.status.busy": "2025-02-04T20:03:27.521043Z",
     "iopub.status.idle": "2025-02-04T20:03:27.530007Z",
     "shell.execute_reply": "2025-02-04T20:03:27.528948Z"
    },
    "papermill": {
     "duration": 0.015498,
     "end_time": "2025-02-04T20:03:27.531925",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.516427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataCleaning(Target_File):\n",
    "    # Dropped the columns\n",
    "    Target_File = Target_File.drop(['videoDescription','videoLiveBroadcastContent','videoFavoriteCount','videoTags','videoUniqueId','channelIdUnique','channelTitleCheck','channelDescription'],axis=1)\n",
    "    duplicates = Target_File[Target_File.duplicated(keep=False)]  # This will select all duplicates, including the first occurrence\n",
    "    \n",
    "    # display(duplicates)\n",
    "    # print(Target_File.duplicated(subset=['videoId', 'channelId']).sum())  # Check for duplicates based on videoId and channelId\n",
    "    \n",
    "    # Removing Duplicates\n",
    "    Target_File  = Target_File.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Remving the videos which has videoDefaultAudioLanguage as None or starts without en\n",
    "    Target_File_EN = Target_File[Target_File['videoDefaultAudioLanguage'].str.startswith(\"en\",na=False)].reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(Target_File_EN['channelName'])):\n",
    "        try:\n",
    "            # Check and translate non-ASCII characters\n",
    "            if not Target_File_EN['channelName'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'channelName'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['channelName'][i])\n",
    "            if not Target_File_EN['videoTitle'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'videoTitle'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['videoTitle'][i])\n",
    "    \n",
    "            # Remove emojis\n",
    "            Target_File_EN.loc[i, 'channelName'] = emoji.replace_emoji(Target_File_EN['channelName'][i], replace='')\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = emoji.replace_emoji(Target_File_EN['videoTitle'][i], replace='')\n",
    "    \n",
    "            # Decode HTML entities like &amp; and &#39;\n",
    "            Target_File_EN.loc[i, 'channelName'] = html.unescape(Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = html.unescape(Target_File_EN['videoTitle'][i])\n",
    "    \n",
    "            # Remove non-ASCII characters\n",
    "            Target_File_EN.loc[i, 'channelName'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['videoTitle'][i])\n",
    "    \n",
    "            # print(Target_File_EN['channelName'][i])\n",
    "            # print(Target_File_EN['videoTitle'][i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Removing Duplicates\n",
    "    Target_File_EN  = Target_File_EN.drop_duplicates(ignore_index=True)\n",
    "    Target_File_EN = Target_File_EN.drop(['videoDefaultLanguage'],axis=1)\n",
    "    Target_File_EN['channelCountry'] = Target_File_EN['channelCountry'].fillna('Unknown')\n",
    "    return Target_File_EN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff38ddfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.541075Z",
     "iopub.status.busy": "2025-02-04T20:03:27.540745Z",
     "iopub.status.idle": "2025-02-04T20:03:27.546203Z",
     "shell.execute_reply": "2025-02-04T20:03:27.545256Z"
    },
    "papermill": {
     "duration": 0.012011,
     "end_time": "2025-02-04T20:03:27.547979",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.535968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def videoDurationClassification(videoDurationInSeconds):\n",
    "    if 0 <= videoDurationInSeconds <= 60:\n",
    "        return \"Very Short\"\n",
    "    elif 61 <= videoDurationInSeconds <= 120:\n",
    "        return \"Short\"\n",
    "    elif 121 <= videoDurationInSeconds <= 300:\n",
    "        return \"Medium\"\n",
    "    elif 301 <= videoDurationInSeconds <= 600:\n",
    "        return \"Long\"\n",
    "    elif 601 <= videoDurationInSeconds <= 3600:\n",
    "        return \"Very Long\"\n",
    "    elif 3601 <= videoDurationInSeconds <= 10800:\n",
    "        return \"Extended\"\n",
    "    elif videoDurationInSeconds > 10800:\n",
    "        return \"Ultra Long\"\n",
    "    else:\n",
    "        return \"Invalid video duration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf36c5",
   "metadata": {
    "papermill": {
     "duration": 0.003513,
     "end_time": "2025-02-04T20:03:27.555306",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.551793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Channel Growth Calculation\n",
    "\n",
    "## **Overview**\n",
    "The channel growth metric is designed to assess the growth of a YouTube channel based on key engagement indicators: \n",
    "- **Channel View Count**\n",
    "- **Channel Subscriber Count**\n",
    "- **Channel Video Count**\n",
    "- **Channel Age (in years)**\n",
    "\n",
    "## **Formula**\n",
    "The channel growth score is computed as:\n",
    "\n",
    "```python\n",
    "channel_growth = ((normalized_views * weight_views) + \n",
    "                  (normalized_subscribers * weight_subscribers) + \n",
    "                  (normalized_videos * weight_videos)) / channel_age_in_years\n",
    "```\n",
    "\n",
    "where:\n",
    "- **Min-Max Normalization** is applied to views, subscribers, and video count:\n",
    "  ```python\n",
    "  normalized_value = (value - min_value) / (max_value - min_value)\n",
    "  ```\n",
    "- **Weighting factors** determine the relative importance of each metric:\n",
    "  - `weight_views = 0.5`\n",
    "  - `weight_subscribers = 0.3`\n",
    "  - `weight_videos = 0.2`\n",
    "- **Channel Age (years)** is computed from:\n",
    "  ```python\n",
    "  channel_age_in_years = (current_timestamp - channelPublishedOnInSeconds) / (365 * 24 * 60 * 60)\n",
    "  ```\n",
    "\n",
    "## **Concepts Used**\n",
    "1. **Min-Max Normalization**: Scales values between 0 and 1.\n",
    "   - [Feature Scaling (Wikipedia)](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "2. **Weighted Scoring**: Prioritizes key metrics based on impact.\n",
    "   - [Weighted Scoring Model](https://theproductmanager.com/topics/weighted-scoring-model/)\n",
    "3. **YouTube Analytics Metrics**: Defines the importance of views, subscribers, and videos.\n",
    "   - [YouTube Analytics Help](https://support.google.com/youtube/answer/9002587?hl=en)\n",
    "4. **Channel Age Calculation**: Determines the time span since the channel was created.\n",
    "   - [Unix Time Conversion](https://www.unixtimestamp.com/)\n",
    "\n",
    "## **Implementation Example**\n",
    "```python\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def calculate_channel_growth(view_count, subscriber_count, video_count, channel_published_on):\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone)\n",
    "    channel_age_in_years = (current_ist_time - channel_published_on) / (365 * 24 * 60 * 60)\n",
    "    \n",
    "    # Min-Max normalization (Assume predefined min/max values from dataset)\n",
    "    min_views, max_views = 1000, 10000000\n",
    "    min_subscribers, max_subscribers = 10, 1000000\n",
    "    min_videos, max_videos = 1, 10000\n",
    "    \n",
    "    normalized_views = (view_count - min_views) / (max_views - min_views)\n",
    "    normalized_subscribers = (subscriber_count - min_subscribers) / (max_subscribers - min_subscribers)\n",
    "    normalized_videos = (video_count - min_videos) / (max_videos - min_videos)\n",
    "    \n",
    "    # Weighted sum\n",
    "    growth_score = ((normalized_views * 0.5) + (normalized_subscribers * 0.3) + (normalized_videos * 0.2)) / channel_age_in_years\n",
    "    \n",
    "    return growth_score\n",
    "```\n",
    "\n",
    "## **Conclusion**\n",
    "This approach helps in analyzing a YouTube channel's growth potential by factoring in **engagement, longevity, and content volume**. It provides a scalable and adaptable framework for evaluating growth trends over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fa5fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.564643Z",
     "iopub.status.busy": "2025-02-04T20:03:27.564234Z",
     "iopub.status.idle": "2025-02-04T20:03:27.569187Z",
     "shell.execute_reply": "2025-02-04T20:03:27.568014Z"
    },
    "papermill": {
     "duration": 0.011691,
     "end_time": "2025-02-04T20:03:27.571037",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.559346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Min-Max normalization\n",
    "def normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f31d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.580087Z",
     "iopub.status.busy": "2025-02-04T20:03:27.579733Z",
     "iopub.status.idle": "2025-02-04T20:03:27.585314Z",
     "shell.execute_reply": "2025-02-04T20:03:27.584091Z"
    },
    "papermill": {
     "duration": 0.012084,
     "end_time": "2025-02-04T20:03:27.587046",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.574962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to handle different datetime formats\n",
    "def parse_datetime(value):\n",
    "    if \"Z\" in value and \".\" not in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    elif \"Z\" in value and \".\" in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    else:\n",
    "        return pd.NaT  # return NaT if neither format matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9852eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.595934Z",
     "iopub.status.busy": "2025-02-04T20:03:27.595601Z",
     "iopub.status.idle": "2025-02-04T20:03:27.604216Z",
     "shell.execute_reply": "2025-02-04T20:03:27.603042Z"
    },
    "papermill": {
     "duration": 0.015362,
     "end_time": "2025-02-04T20:03:27.606256",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.590894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_channel_growth(Cleaned_File):\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone).replace(tzinfo=None) \n",
    "    # print(current_ist_time)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    channel\n",
    "    \n",
    "    '''\n",
    "\n",
    "    channelPublishedOn = Cleaned_File[\"channelPublishedOn\"].apply(parse_datetime)\n",
    "    # print(Cleaned_File[\"channelPublishedOn\"])\n",
    "    \n",
    "    Cleaned_File['channelAgeInYears'] = (current_ist_time - channelPublishedOn).dt.total_seconds() / (365 * 24 * 60 * 60)\n",
    "    # print(Cleaned_File['channelAgeInYears'])\n",
    "    # Min-Max normalization\n",
    "    Cleaned_File['channelNormalizedViewCount'] = normalize(Cleaned_File['channelViewCount'])\n",
    "    Cleaned_File['channelNormalizedSubscriberCount'] = normalize(Cleaned_File['channelSubscriberCount'])\n",
    "    Cleaned_File['channelNormalizedVideoCount'] = normalize(Cleaned_File['channelVideoCount'])\n",
    "    Cleaned_File['channelNormalizedChannelAge'] = normalize(Cleaned_File['channelAgeInYears'])\n",
    "    \n",
    "    # Weights\n",
    "    weight_views = 0.5\n",
    "    weight_subscribers = 0.3\n",
    "    weight_videos = 0.2\n",
    "    \n",
    "    # Growth Score Calculation\n",
    "    Cleaned_File['channelGrowthScore'] = ((Cleaned_File['channelNormalizedViewCount'] * weight_views) +\n",
    "                          (Cleaned_File['channelNormalizedSubscriberCount'] * weight_subscribers) +\n",
    "                          (Cleaned_File['channelNormalizedVideoCount'] * weight_videos)) / Cleaned_File['channelNormalizedChannelAge']\n",
    "\n",
    "    '''\n",
    "\n",
    "    Video\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    videoPublishedOn = Cleaned_File[\"videoPublishedOn\"].apply(parse_datetime)\n",
    "    \n",
    "    # Compute video age\n",
    "    Cleaned_File[\"videoAgeInDays\"] = (current_ist_time - videoPublishedOn).dt.total_seconds() / (24 * 60 * 60)\n",
    "    \n",
    "    # Calculate engagement metrics\n",
    "    Cleaned_File[\"videoViewsPerDay\"] = Cleaned_File[\"videoViewCount\"] / Cleaned_File[\"videoAgeInDays\"]\n",
    "    Cleaned_File[\"videoLikeToViewRatio\"] = Cleaned_File[\"videoLikeCount\"] / Cleaned_File[\"videoViewCount\"]\n",
    "    Cleaned_File[\"videoCommentToViewRatio\"] = Cleaned_File[\"videoCommentCount\"] / Cleaned_File[\"videoViewCount\"]\n",
    "    \n",
    "    # Engagement Score Calculation\n",
    "    Cleaned_File[\"videoEngagementScore\"] = (\n",
    "        (Cleaned_File[\"videoViewsPerDay\"] * 0.5) +\n",
    "        (Cleaned_File[\"videoLikeToViewRatio\"] * 100 * 0.3) +\n",
    "        (Cleaned_File[\"videoCommentToViewRatio\"] * 100 * 0.2)\n",
    "    )\n",
    "    \n",
    "    return Cleaned_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac858bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.615400Z",
     "iopub.status.busy": "2025-02-04T20:03:27.615012Z",
     "iopub.status.idle": "2025-02-04T20:03:27.621546Z",
     "shell.execute_reply": "2025-02-04T20:03:27.620598Z"
    },
    "papermill": {
     "duration": 0.012969,
     "end_time": "2025-02-04T20:03:27.623211",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.610242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FeatureEngineering(Cleaned_File):\n",
    "    # Feature Engineering  \n",
    "    \n",
    "    # videoPublishedWeekDay - Derive the day of the week from the videoPublishedOn timestamp.  \n",
    "    Cleaned_File['videoPublishedWeekDay'] = pd.to_datetime(Cleaned_File[\"videoPublishedOn\"]).dt.day_name()\n",
    "    # videoDurationClassification - Categorize videos into duration segments based on videoDurationInSeconds.  \n",
    "        # Categories:\n",
    "        #     Very Short (0 - 60 sec), Typically Shorts, Reels, or quick snippets.\n",
    "        #     Short (61 sec - 2 min), Brief content, short tutorials, or quick explanations.\n",
    "        #     Medium (2 min 1 sec - 5 min), Standard short-form content, concise videos.\n",
    "        #     Long (5 min 1 sec - 10 min), In-depth discussions, detailed tutorials.\n",
    "        #     Very Long (10 min 1 sec - 1 hour), Educational content, podcasts, detailed explainers.\n",
    "        #     Extended (1 hour 1 sec - 3 hours), Webinars, live sessions, long-form discussions.\n",
    "        #     Ultra Long (3 hours 1 sec and above), Movie-length content, streams, recorded conferences.\n",
    "    Cleaned_File['videoDurationClassification'] = Cleaned_File['videoDurationInSeconds'].apply(videoDurationClassification)\n",
    "    \n",
    "    # channelGrowth metric and normalization of columns(channelPublishedOn, channelViewCount, channelSubscriberCount, and channelVideoCount) - Assess channel growth using factors such as channelPublishedOn, channelViewCount, channelSubscriberCount, and channelVideoCount.  \n",
    "    # videoEngagementScore metric and normalization of columns(videoPublishedOn, videoViewCount, videoLikeCount, and videoCommentCount)- Evaluate video engagement based on videoPublishedOn, videoViewCount, videoLikeCount, and videoCommentCount.  \n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)\n",
    "    \n",
    "    # channelGrowthScoreRank - Rank channels based on the channelGrowth metric.  \n",
    "    Cleaned_File[\"channelGrowthScoreRank\"] = Cleaned_File[\"channelGrowthScore\"].rank()\n",
    "    # videoEngagementScoreRank - Rank videos based on the videoEngagement metric.  \n",
    "    Cleaned_File[\"videoEngagementScoreRank\"] = Cleaned_File[\"videoEngagementScore\"].rank()\n",
    "    # Geographic Classification - Assign an upper-level geographical classification.  \n",
    "        # Columns include:  \n",
    "            # - country_name  \n",
    "            # - continent  \n",
    "            # - continent_code  \n",
    "            # - it_hub_country (indicator for whether the country is a major IT hub).  \n",
    "\n",
    "    Country_Details_ISO = Requirement_File_Extraction(repo_url,kaggle_repo_url).transpose()\n",
    "    Country_Details_ISO = Country_Details_ISO.reset_index()\n",
    "    Country_Details_ISO.rename(columns={'index': 'country_code'}, inplace=True)\n",
    "    resultDataFrame = pd.merge(Cleaned_File, Country_Details_ISO, left_on='channelCountry', right_on='country_code', how='left')\n",
    "    resultDataFrame.fillna('Unknown', inplace=True)\n",
    "    # for i in Country_Details_ISO['country_name']:\n",
    "    #     print(i)\n",
    "    # display(Country_Details_ISO)\n",
    "    return resultDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cdac6f",
   "metadata": {
    "_cell_guid": "c54e2df3-a441-4cc0-a5e9-504daac1bb7b",
    "_uuid": "e6dd150b-70d1-4e52-8c84-600926543736",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.632063Z",
     "iopub.status.busy": "2025-02-04T20:03:27.631707Z",
     "iopub.status.idle": "2025-02-04T20:03:27.638549Z",
     "shell.execute_reply": "2025-02-04T20:03:27.637320Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013558,
     "end_time": "2025-02-04T20:03:27.640577",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.627019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GitHubPush(Target_File_EN):\n",
    "    record_count = len(Target_File_EN)\n",
    "    \n",
    "    # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    \n",
    "    # Create a filename using the generated timestamp to ensure uniqueness with number of records.\n",
    "    filename = f\"DC_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame to a JSON file with readable formatting.\n",
    "    Target_File_EN.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        # Create the destination directory if it doesn't exist\n",
    "        os.makedirs(destination_path)\n",
    "        print('created the destination directory, DataCleaning/Daily')\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    else:\n",
    "        print('Destination directory already exists')\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the repository for git operations\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the staging area\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a timestamp for the commit message\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    # Commit the changes with a message including the timestamp\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the changes to the remote repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    push_result = origin.push()\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac66db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.649797Z",
     "iopub.status.busy": "2025-02-04T20:03:27.649425Z",
     "iopub.status.idle": "2025-02-04T20:03:27.654160Z",
     "shell.execute_reply": "2025-02-04T20:03:27.652885Z"
    },
    "papermill": {
     "duration": 0.011691,
     "end_time": "2025-02-04T20:03:27.656102",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.644411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    Source_File = Source_File_Extraction(repo_url,kaggle_repo_url)\n",
    "    Cleaned_File = DataCleaning(Source_File)\n",
    "    # display(Cleaned_File.sort_values(by='videoDurationInSeconds',ascending = True))\n",
    "    Feature_File = FeatureEngineering(Cleaned_File)\n",
    "    # display(Feature_File)\n",
    "    GitHubPush(Feature_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91586d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:03:27.664804Z",
     "iopub.status.busy": "2025-02-04T20:03:27.664461Z",
     "iopub.status.idle": "2025-02-04T20:03:36.339943Z",
     "shell.execute_reply": "2025-02-04T20:03:36.338629Z"
    },
    "papermill": {
     "duration": 8.681966,
     "end_time": "2025-02-04T20:03:36.341899",
     "exception": false,
     "start_time": "2025-02-04T20:03:27.659933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned the git repo\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "DataFrame saved as DC_2025-02-05_01:33:35_412_records.json\n",
      "Destination directory already exists\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os  \n",
    "    import git \n",
    "    from git import Repo  \n",
    "    import time\n",
    "    import datetime  \n",
    "    from pytz import timezone\n",
    "    import pytz\n",
    "    import pandas as pd\n",
    "    import deep_translator\n",
    "    from deep_translator import GoogleTranslator\n",
    "    import shutil\n",
    "    import emoji\n",
    "    import re\n",
    "    import html\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"dataCleanRepoUrl\")\n",
    "    repo_url = secret_value_0\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    kaggle_repo_url = '/kaggle/working/YouTubeFoodChannelAnalysis'\n",
    "    destination_path = '/kaggle/working/YouTubeFoodChannelAnalysis/DataCleaning/Daily'\n",
    "    source_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Source/Daily'\n",
    "    requirement_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Requirement/Daily'\n",
    "    # Below script prevents all columns and rows from getting truncated while display\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.max_rows\",None)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.393857,
   "end_time": "2025-02-04T20:03:36.967021",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-04T20:03:24.573164",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
