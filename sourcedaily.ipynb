{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07f8af8",
   "metadata": {
    "_cell_guid": "a966fe56-66f1-4472-9f7d-87b57c211ea8",
    "_uuid": "b54f2e82-54ff-4df2-a6c6-7481fcec10df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.748514Z",
     "iopub.status.busy": "2025-01-28T19:41:47.748095Z",
     "iopub.status.idle": "2025-01-28T19:41:47.756549Z",
     "shell.execute_reply": "2025-01-28T19:41:47.755536Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015787,
     "end_time": "2025-01-28T19:41:47.758610",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.742823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def VideoDetailExtraction(kw_list, maxResults=50):\n",
    "    \"\"\"\n",
    "    Fetches a list of video details from YouTube based on the given keyword(s) for the initial batch.\n",
    "\n",
    "    Args:\n",
    "        kw_list (str): The keyword(s) to search for.\n",
    "        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response containing video details. Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the API request to fetch video details\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n",
    "            order='viewCount',         # Order results by view count\n",
    "            q=kw_list,                 # Search query\n",
    "            relevanceLanguage='en',    # Limit results to English-relevant videos\n",
    "            type='video',              # Restrict results to videos only\n",
    "            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n",
    "            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n",
    "            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n",
    "            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n",
    "        )\n",
    "\n",
    "        # Execute the API request\n",
    "        response = request.execute()\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any errors encountered during the API call\n",
    "        print(f\"Error during VideoDetailExtraction(): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def VideoDetailExtractionNextPageToken(kw_list, nextPageToken, maxResults=50):\n",
    "    \"\"\"\n",
    "    Fetches the next page of video details from YouTube using a continuation token.\n",
    "\n",
    "    Args:\n",
    "        kw_list (str): The keyword(s) to search for.\n",
    "        nextPageToken (str): The token for fetching the next page of results.\n",
    "        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response containing video details for the next page. Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the API request to fetch the next page of video details\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n",
    "            order='viewCount',         # Order results by view count\n",
    "            q=kw_list,                 # Search query\n",
    "            relevanceLanguage='en',    # Limit results to English-relevant videos\n",
    "            type='video',              # Restrict results to videos only\n",
    "            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n",
    "            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n",
    "            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n",
    "            pageToken=nextPageToken,   # Token for fetching the next page\n",
    "            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n",
    "        )\n",
    "\n",
    "        # Execute the API request\n",
    "        response = request.execute()\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any errors encountered during the API call\n",
    "        print(f\"Error during VideoDetailExtractionNextPageToken(): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b3fea1",
   "metadata": {
    "_cell_guid": "7ed47aef-2670-4fcf-a35e-854613a2a456",
    "_uuid": "7e659bdd-62eb-4544-bb45-516dde62bb35",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.767120Z",
     "iopub.status.busy": "2025-01-28T19:41:47.766780Z",
     "iopub.status.idle": "2025-01-28T19:41:47.791144Z",
     "shell.execute_reply": "2025-01-28T19:41:47.790162Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030726,
     "end_time": "2025-01-28T19:41:47.793020",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.762294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def VideoDataFrame(response):\n",
    "    \"\"\"\n",
    "    Processes video and channel details from the YouTube API response, structures the data into DataFrames,\n",
    "    and merges them to create a comprehensive dataset.\n",
    "\n",
    "    Args:\n",
    "        response (dict): The response object returned by the YouTube API containing video details.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: A DataFrame containing merged video and channel details.\n",
    "            - str or None: The next page token if available, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize lists to store video and channel details\n",
    "        videoDetails = []\n",
    "        videoIds = []\n",
    "        channelIds = []\n",
    "        channelDetails = []\n",
    "        \n",
    "        '''\n",
    "        Video Search Block: Extract basic video details from the response.\n",
    "        '''\n",
    "        for i in range(len(response['items'])):\n",
    "            # Extract publication time and convert to components\n",
    "            publishedOn = response['items'][i].get('snippet', '0000-00-00T00:00:00Z').get('publishTime', '0000-00-00T00:00:00Z')\n",
    "            publishTime = re.split(r'[TZ-]', publishedOn)\n",
    "            total_seconds = 0\n",
    "            if publishedOn != '0000-00-00T00:00:00Z':\n",
    "                try:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except ValueError:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                epoch = datetime.datetime(1970, 1, 1)\n",
    "                total_seconds = int((dt - epoch).total_seconds())\n",
    "            \n",
    "            # Append extracted video details\n",
    "            videoDetails.append({\n",
    "                'channelId': response['items'][i]['snippet']['channelId'],\n",
    "                'channelName': response['items'][i]['snippet']['channelTitle'],\n",
    "                'videoId': response['items'][i]['id']['videoId'],\n",
    "                'videoTitle': response['items'][i]['snippet']['title'],\n",
    "                'videoPublishYear': publishTime[0],  # Extracted year\n",
    "                'videoPublishMonth': publishTime[1],  # Extracted month\n",
    "                'videoPublishDay': publishTime[2],  # Extracted day\n",
    "                'videoPublishTime': publishTime[3],  # Extracted time\n",
    "                'videoPublishedOn': publishedOn,\n",
    "                'videoPublishedOnInSeconds': total_seconds\n",
    "            })\n",
    "            \n",
    "            # Collect video and channel IDs\n",
    "            videoIds.append(response['items'][i]['id']['videoId'])\n",
    "            channelIds.append(response['items'][i]['snippet']['channelId'])\n",
    "        \n",
    "        # Extract next page token if available\n",
    "        nextPageToken = response.get(\"nextPageToken\", None)\n",
    "        \n",
    "        '''\n",
    "        Video Block: Fetch additional details about each video using its ID.\n",
    "        '''\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part='id,statistics,snippet,contentDetails,localizations,status,liveStreamingDetails,paidProductPlacementDetails,player,recordingDetails,topicDetails',\n",
    "                id=videoIds\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during the API call\n",
    "            print(f\"Error during videos().list(): {e}\")\n",
    "            return None\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            video = response['items'][i]\n",
    "\n",
    "            # Video id\n",
    "            videoDetails[i]['videoUniqueId'] = video.get('id',None)\n",
    "            \n",
    "            # Video statistics\n",
    "            statistics = video.get('statistics', {})\n",
    "            videoDetails[i]['videoViewCount'] = statistics.get('viewCount', 0)\n",
    "            videoDetails[i]['videoLikeCount'] = statistics.get('likeCount', 0)\n",
    "            videoDetails[i]['videoFavoriteCount'] = statistics.get('favoriteCount', 0)\n",
    "            videoDetails[i]['videoCommentCount'] = statistics.get('commentCount', 0)\n",
    "            \n",
    "            # Video snippet details\n",
    "            snippet = video.get('snippet', {})\n",
    "            videoDetails[i]['videoDescription'] = snippet.get('description', None)\n",
    "            videoDetails[i]['videoTags'] = snippet.get('tags', [])\n",
    "            videoDetails[i]['videoCategoryId'] = snippet.get('categoryId', None)\n",
    "            videoDetails[i]['videoLiveBroadcastContent'] = snippet.get('liveBroadcastContent', None)\n",
    "            videoDetails[i]['videoDefaultLanguage'] = snippet.get('defaultLanguage', None)\n",
    "            videoDetails[i]['videoDefaultAudioLanguage'] = snippet.get('defaultAudioLanguage', None)\n",
    "            \n",
    "            # Video duration (convert ISO 8601 to seconds)\n",
    "            duration = video.get('contentDetails', {}).get('duration', None)\n",
    "            if duration:\n",
    "                # Match ISO 8601 duration format\n",
    "                match = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", duration)\n",
    "                \n",
    "                if match:\n",
    "                    # Extract hours, minutes, and seconds; fallback to 0 if missing\n",
    "                    hours = int(match.group(1) or 0)\n",
    "                    minutes = int(match.group(2) or 0)\n",
    "                    seconds = int(match.group(3) or 0)\n",
    "                    \n",
    "                    # Calculate total duration in seconds\n",
    "                    total_duration_in_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "                    \n",
    "                    # Assign duration and classify content type\n",
    "                    videoDetails[i]['videoDuration'] = duration\n",
    "                    videoDetails[i]['videoDurationInSeconds'] = total_duration_in_seconds\n",
    "                    \n",
    "                    if total_duration_in_seconds <= 60:\n",
    "                        videoDetails[i]['ContentType'] = 'Short'\n",
    "                    elif total_duration_in_seconds <= 90 and video.get('snippet', {}).get('liveBroadcastContent', '') == 'short':\n",
    "                        videoDetails[i]['ContentType'] = 'Short'\n",
    "                    else:\n",
    "                        videoDetails[i]['ContentType'] = 'Video'\n",
    "                else:\n",
    "                    # Handle invalid duration format\n",
    "                    videoDetails[i]['videoDuration'] = None\n",
    "                    videoDetails[i]['videoDurationInSeconds'] = None\n",
    "                    videoDetails[i]['ContentType'] = 'Unknown'\n",
    "            else:\n",
    "                # If duration is missing\n",
    "                videoDetails[i]['videoDuration'] = None\n",
    "                videoDetails[i]['videoDurationInSeconds'] = None\n",
    "                videoDetails[i]['ContentType'] = 'Unknown'\n",
    "    \n",
    "            # Additional video details\n",
    "            content_details = video.get('contentDetails', {})\n",
    "            videoDetails[i]['videoDimension'] = content_details.get('dimension', None)\n",
    "            videoDetails[i]['videoDefinition'] = content_details.get('definition', None)\n",
    "            videoDetails[i]['videoCaption'] = content_details.get('caption', None)\n",
    "            videoDetails[i]['videoLicensedContent'] = content_details.get('licensedContent', False)\n",
    "            videoDetails[i]['videoProjection'] = content_details.get('projection', False)\n",
    "        \n",
    "        '''\n",
    "        Channel Block: Fetch details for channels associated with the videos.\n",
    "        '''\n",
    "        videoDetails = pd.DataFrame(videoDetails)\n",
    "        Unique_ChannelIds = list(set(videoDetails['channelId']))\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='id,contentDetails,brandingSettings,contentOwnerDetails,localizations,snippet,statistics,status,topicDetails',\n",
    "                id=Unique_ChannelIds\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during the API call\n",
    "            print(f\"Error during channels().list(): {e}\")\n",
    "            return None\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            item = response['items'][i]\n",
    "            snippet = item.get('snippet', {})\n",
    "            publishedOn = snippet.get('publishedAt', '0000-00-00T00:00:00Z')\n",
    "            publishedAt = re.split(r'[TZ-]', publishedOn)\n",
    "            total_seconds = 0\n",
    "            if publishedOn != '0000-00-00T00:00:00Z':\n",
    "                try:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except ValueError:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                epoch = datetime.datetime(1970, 1, 1)\n",
    "                total_seconds = int((dt - epoch).total_seconds())\n",
    "            \n",
    "            # Extract channel details\n",
    "            channelDetails.append({\n",
    "                'channelIdUnique': item['id'],\n",
    "                'channelTitleCheck': snippet.get('title', None),\n",
    "                'channelDescription': snippet.get('description', None),\n",
    "                'channelCustomUrl': snippet.get('customUrl', None),\n",
    "                'channelPublishYear': publishedAt[0],\n",
    "                'channelPublishMonth': publishedAt[1],\n",
    "                'channelPublishDay': publishedAt[2],\n",
    "                'channelPublishTime': publishedAt[3],\n",
    "                'channelPublishedOn': publishedOn,\n",
    "                'channelPublishedOnInSeconds': total_seconds,\n",
    "                'channelCountry': snippet.get('country', None),\n",
    "                'channelViewCount': item.get('statistics', {}).get('viewCount', 0),\n",
    "                'channelSubscriberCount': item.get('statistics', {}).get('subscriberCount', 0),\n",
    "                'channelVideoCount': item.get('statistics', {}).get('videoCount', 0),\n",
    "            })\n",
    "        \n",
    "        # Convert channel details to DataFrame\n",
    "        channelDetails = pd.DataFrame(channelDetails)\n",
    "        \n",
    "        '''\n",
    "        Result: Merge video and channel details into a single DataFrame.\n",
    "        '''\n",
    "        resultDataFrame = pd.merge(videoDetails, channelDetails, left_on='channelId', right_on='channelIdUnique', how='left')\n",
    "        return resultDataFrame, nextPageToken\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing VideoDataFrame(): {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea577d00",
   "metadata": {
    "_cell_guid": "22f805db-4d55-4099-aa5c-b9a2a153a42b",
    "_uuid": "ea0ccceb-a350-46a5-8bce-6c1b0f84419d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.800911Z",
     "iopub.status.busy": "2025-01-28T19:41:47.800567Z",
     "iopub.status.idle": "2025-01-28T19:41:47.810625Z",
     "shell.execute_reply": "2025-01-28T19:41:47.809384Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015967,
     "end_time": "2025-01-28T19:41:47.812357",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.796390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def VideoDetailsStructuring(max_record_count, kw_list):\n",
    "    \"\"\"\n",
    "    Fetches and structures video details into a DataFrame, handling pagination if necessary.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of video records to fetch.\n",
    "        kw_list (str): The keyword(s) to use for fetching video details.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing video details. Returns an empty DataFrame on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize an empty DataFrame to store results\n",
    "        resultDataFrame = pd.DataFrame()\n",
    "\n",
    "        # Initialize the nextPageToken for pagination\n",
    "        nextPageToken = None\n",
    "\n",
    "        # Define the batch sizes for video fetching\n",
    "        record_fetching_batches = [50]  # Default batch size for YouTube API requests\n",
    "\n",
    "        # Adjust the batch sizes based on the max_record_count\n",
    "        if max_record_count > 50:\n",
    "            quotient = max_record_count // 50  # Number of full batches\n",
    "            remainder = [max_record_count % 50]  # Remaining records in the last batch\n",
    "            record_fetching_batches = record_fetching_batches * quotient\n",
    "            if remainder[0] > 0:\n",
    "                record_fetching_batches.extend(remainder)  # Add the remainder as a batch\n",
    "        else:\n",
    "            record_fetching_batches = [max_record_count]  # Single batch if max_record_count <= 50\n",
    "\n",
    "        # Case 1: Only one batch needed\n",
    "        if len(record_fetching_batches) == 1:\n",
    "            # Fetch video details for the single batch\n",
    "            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n",
    "            if response is None:\n",
    "                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Process the response into a DataFrame and get the nextPageToken\n",
    "            resultDataFrame, nextPageToken = VideoDataFrame(response)\n",
    "            nextPageToken = None  # Reset the token as no further pages are needed\n",
    "            if resultDataFrame is None:\n",
    "                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "            return resultDataFrame\n",
    "\n",
    "        # Case 2: Multiple batches needed\n",
    "        elif len(record_fetching_batches) > 1:\n",
    "            # Fetch initial batch of video details\n",
    "            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n",
    "            if response is None:\n",
    "                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Process the response into a DataFrame and get the nextPageToken\n",
    "            resultDataFrame, nextPageToken = VideoDataFrame(response)\n",
    "            if resultDataFrame is None:\n",
    "                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Loop through subsequent batches\n",
    "            for batch in record_fetching_batches[1:]:\n",
    "                # Fetch details for the next batch using nextPageToken\n",
    "                response = VideoDetailExtractionNextPageToken(kw_list, nextPageToken, batch)\n",
    "                if response is None:\n",
    "                    print(\"Failed to fetch next page of video details - VideoDetailExtractionNextPageToken() returned None, hence returned till now fetched videoDetails.\")\n",
    "                    break\n",
    "\n",
    "                # Process the response into a DataFrame\n",
    "                resultDataFrame_next, nextPageToken = VideoDataFrame(response)\n",
    "                if resultDataFrame_next is not None:\n",
    "                    # Concatenate the new DataFrame to the result DataFrame\n",
    "                    resultDataFrame = pd.concat([resultDataFrame, resultDataFrame_next], ignore_index=True)\n",
    "\n",
    "                # Break the loop if we've reached the max record count or no more pages are available\n",
    "                if len(resultDataFrame) >= max_record_count or not nextPageToken:\n",
    "                    break\n",
    "\n",
    "        return resultDataFrame  # Return the final result DataFrame\n",
    "    except Exception as e:\n",
    "        print(f\"Error during VideoDetailsStructuring(), hence returned empty DataFrame: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0710a1a",
   "metadata": {
    "_cell_guid": "fb756fad-2e8f-4da9-b761-8562ad042a29",
    "_uuid": "2309f766-37cf-4cdf-897b-fab2f69b6969",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.820618Z",
     "iopub.status.busy": "2025-01-28T19:41:47.820230Z",
     "iopub.status.idle": "2025-01-28T19:41:47.826080Z",
     "shell.execute_reply": "2025-01-28T19:41:47.825021Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011832,
     "end_time": "2025-01-28T19:41:47.827855",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.816023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RawFile(max_record_count):\n",
    "    \"\"\"\n",
    "    Processes video details, structures the data, and saves it as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of records to process.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is successfully created and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call the function to structure video details and return a DataFrame.\n",
    "        # `kw_list` is assumed to be a global variable containing the search keyword(s).\n",
    "        dataframe = VideoDetailsStructuring(max_record_count, kw_list)\n",
    "        \n",
    "        # Check if the DataFrame is not empty before saving.\n",
    "        if not dataframe.empty:\n",
    "            # Count the number of records (rows) in the DataFrame\n",
    "            record_count = len(dataframe)\n",
    "            \n",
    "            # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "            timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        \n",
    "            # Create a filename using the generated timestamp to ensure uniqueness with number of records.\n",
    "            filename = f\"S_{timestamp}_{record_count}_records.json\"\n",
    "            \n",
    "            # Save the DataFrame to a JSON file with readable formatting.\n",
    "            dataframe.to_json(filename, orient=\"records\", indent=4)\n",
    "            print(f\"DataFrame saved as {filename}\")\n",
    "        else:\n",
    "            # Log a message if the DataFrame is empty.\n",
    "            print(\"No data to save since empty DataFrame returned.\")\n",
    "        \n",
    "        # Return True indicating the process was successful.\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Handle and log any errors that occur during the process.\n",
    "        print(f\"Error during raw file creation: {e}\")\n",
    "        \n",
    "        # Return False indicating the process failed.\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37095b14",
   "metadata": {
    "_cell_guid": "b49aaf9b-31ba-4270-bc3e-6fb1d57dc70a",
    "_uuid": "6c62485a-f5b5-45c0-8181-21326d93b97a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.837841Z",
     "iopub.status.busy": "2025-01-28T19:41:47.837352Z",
     "iopub.status.idle": "2025-01-28T19:41:47.846753Z",
     "shell.execute_reply": "2025-01-28T19:41:47.845594Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015487,
     "end_time": "2025-01-28T19:41:47.848670",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.833183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PushToGithub():\n",
    "    \"\"\"\n",
    "    Automates the process of identifying the latest .json file, copying it \n",
    "    to a GitHub repository, and pushing the changes to the same branch.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the process completes successfully and the file is pushed to GitHub, \n",
    "              False if an error occurs during any step.\n",
    "    \"\"\"\n",
    "    # List all files in the working directory\n",
    "    output_files = os.listdir('/kaggle/working')\n",
    "    \n",
    "    try:\n",
    "        # Filter and find the most recent .json file\n",
    "        json_files = [file for file in output_files if file.startswith(\"S_\") and file.endswith(\"_records.json\")]\n",
    "        if json_files:\n",
    "            LatestFiles = max(json_files, key=os.path.getctime)  # Get the latest file based on creation time\n",
    "        else:\n",
    "            raise ValueError(\"No JSON files found!\")  # Raise an error if no JSON files are found\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred at fetching recent .json file: {e}\")\n",
    "        return False  # Exit the function if there's an error in fetching JSON files\n",
    "    \n",
    "    # Define repository and destination paths\n",
    "    kaggle_repo_url = '/kaggle/working/YouTubeFoodChannelAnalysis'\n",
    "    destination_path = '/kaggle/working/YouTubeFoodChannelAnalysis/Source/Daily'\n",
    "\n",
    "    \n",
    "    print(LatestFiles)  # Print the latest JSON file name\n",
    "    try:\n",
    "        # Check if the repository already exists\n",
    "        if os.path.exists(kaggle_repo_url):\n",
    "            print(\"Already cloned and the repo file exists\")\n",
    "            repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "            origin = repo.remote(name='origin')  # Get the remote repository\n",
    "            origin.pull()  # Pull the latest changes from the repository\n",
    "            print(\"Successfully pulled the git repo before push\")\n",
    "        else:\n",
    "            # Clone the repository if it doesn't exist\n",
    "            repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "            print(\"Successfully cloned the git repo\")\n",
    "        \n",
    "        # Check if the destination path exists, and copy the latest file\n",
    "        if os.path.exists(destination_path):\n",
    "            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "        else:\n",
    "            # Create the destination directory if it doesn't exist\n",
    "            os.makedirs(destination_path)\n",
    "            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "        \n",
    "        # Initialize the repository for git operations\n",
    "        repo = Repo(kaggle_repo_url)\n",
    "        \n",
    "        # Add the copied file to the staging area\n",
    "        repo.index.add([f\"{destination_path}/{LatestFiles}\"])\n",
    "        \n",
    "        timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        # Commit the changes with a message including the timestamp and file name\n",
    "        repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {LatestFiles}\")\n",
    "        \n",
    "        # Push the changes to the remote repository\n",
    "        origin = repo.remote(name=\"origin\")\n",
    "        push_result = origin.push()\n",
    "        if push_result:\n",
    "            print(\"Output files successfully pushed to GitHub!\")\n",
    "        else:\n",
    "            print(\"Output files pushed to GitHub failed:(\")\n",
    "        return True  # Return True if the process completes successfully\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the git automation process\n",
    "        print(f\"An error occurred at git automation code: {e}\")\n",
    "        return False  # Return False if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70734e6f",
   "metadata": {
    "_cell_guid": "a1724d1c-e537-48de-87e2-7ffd6c746139",
    "_uuid": "270ccbca-4e20-4eb7-aa5b-4454cebed6b2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.856426Z",
     "iopub.status.busy": "2025-01-28T19:41:47.856058Z",
     "iopub.status.idle": "2025-01-28T19:41:47.860671Z",
     "shell.execute_reply": "2025-01-28T19:41:47.859580Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010422,
     "end_time": "2025-01-28T19:41:47.862526",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.852104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(max_record_count):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the execution of raw data extraction and pushing data to GitHub.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of records to process.\n",
    "    \"\"\"\n",
    "    # Call the RawFile function to process and extract raw data.\n",
    "    # This function likely handles fetching data, processing it, and storing it in a file.\n",
    "    RawFile(max_record_count)\n",
    "    \n",
    "    # Call the PushToGithub function to push the processed data to a GitHub repository.\n",
    "    # This function likely handles staging, committing, and pushing the file to the repository.\n",
    "    PushToGithub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28d45db",
   "metadata": {
    "_cell_guid": "719b28a1-2842-429f-81dd-5a5420e73067",
    "_uuid": "3ba944ef-356d-4efd-866d-fb896f0e5044",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-28T19:41:47.870172Z",
     "iopub.status.busy": "2025-01-28T19:41:47.869846Z",
     "iopub.status.idle": "2025-01-28T19:42:00.826820Z",
     "shell.execute_reply": "2025-01-28T19:42:00.824792Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.963702,
     "end_time": "2025-01-28T19:42:00.829531",
     "exception": false,
     "start_time": "2025-01-28T19:41:47.865829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as S_2025-01-29_01:11:57_506_records.json\n",
      "S_2025-01-29_01:11:57_506_records.json\n",
      "Successfully cloned the git repo\n",
      "Output files successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Importing necessary libraries\n",
    "    from googleapiclient.discovery import build  # For interacting with YouTube API\n",
    "    from IPython.display import JSON, display  # For displaying JSON responses in Jupyter Notebooks\n",
    "    import re  # For regular expressions\n",
    "    import datetime  # For date and time manipulations\n",
    "    # from dateutil.relativedelta import relativedelta  # For handling relative date differences\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import os  # For interacting with the operating system\n",
    "    from kaggle_secrets import UserSecretsClient  # For securely managing API keys in Kaggle\n",
    "    import git  # For Git-related operations\n",
    "    from git import Repo  # For working with repositories\n",
    "    import shutil  # For file and directory operations\n",
    "    from pytz import timezone  # For handling time zones\n",
    "    from datetime import timedelta  # For handling time differences\n",
    "    \n",
    "    # Fetching secrets from Kaggle's secure environment\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"sourceApiKey\")  # Fetch the YouTube API key\n",
    "    secret_value_1 = user_secrets.get_secret(\"sourceRepoUrl\")  # Fetch the source repository URL\n",
    "    \n",
    "    # Assigning secrets to variables\n",
    "    api_key = secret_value_0\n",
    "    repo_url = secret_value_1\n",
    "    \n",
    "    # Setting up YouTube API details\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    youtube = build(api_service_name, api_version, developerKey=api_key)  # Initialize YouTube API client\n",
    "    \n",
    "    # Setting the timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Maximum number of records to fetch\n",
    "    max_record_count = 4000\n",
    "    \n",
    "    # Keyword list for searching YouTube videos\n",
    "    kw_list = \"devops\"\n",
    "    \n",
    "    # Call the main function with the maximum record count as an argument\n",
    "    main(max_record_count)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.956151,
   "end_time": "2025-01-28T19:42:01.759854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-28T19:41:44.803703",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
